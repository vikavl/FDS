{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1171fb23",
      "metadata": {
        "id": "1171fb23",
        "tags": []
      },
      "source": [
        "# **#2 Homework: Classification**\n",
        "\n",
        "**Fundamentals of Data Science - Winter Semester 2024**\n",
        "\n",
        "##### Matteo Migliarini (TA), Matteo Rampolla (TA) and Prof. Indro Spinelli\n",
        "<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it>, <spinelli@di.uniroma1.it>\n",
        "\n",
        "---\n",
        "\n",
        "*Note: your task is to fill in the missing code where you see `\"YOUR CODE HERE\"` and the text part `\"WRITE YOUR TEXT HERE\"` part corresponding to each subproblem and produce brief reports on the results whenever necessary. Note also that a part of this missing code is also distributed in the python files in the folder `libs/`*\n",
        "\n",
        "As part of the homework, provide the answer to questions in this notebook report-like manner.\n",
        "\n",
        "After you have implemented all the missing code in the required sections, you will be able to run all the code without any errors.\n",
        "\n",
        "We kindly ask you to double-check this since **all** the delivered homework will be executed.\n",
        "\n",
        "The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "**Submit it** by sending an email to:\n",
        "\n",
        "<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it> and <spinelli@di.uniroma1.it> **by 29th November, 23:59**.\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "**Outline and Scores for #2 Homework:**\n",
        "\n",
        "\n",
        "* **Question 1: Logistic Regression** *(6 points)*\n",
        "  * **1.1: Log-likelihood and Gradient Ascent rule** (1 points)\n",
        "  * **1.2: Implementation of Logistic Regression with Gradient Ascent** (2 points)\n",
        "  * **1.3: Report** (3 points)\n",
        "* **Question 2: Polynomial Expansion** *(7 points)*\n",
        "  * **2.1: Polynomial features for logistic regression** (1 points)\n",
        "  * **2.2: Plot the computed non-linear boundary** (2 point)\n",
        "  * **2.4: Penalization** (4 points)\n",
        "* **Question 3: Multinomial Classification** *(9  points)*\n",
        "  * **3.1: Softmax Regression Model** (1 point)\n",
        "  * **3.2: Coding** (3 points)\n",
        "  * **3.3: Pipeline** (2 point)\n",
        "  * **3.4: Hyperparameters** (1 point)\n",
        "  * **3.5: Report** (2 point)\n",
        "* **Question 4: First approach to CNNs** *(8 points)*\n",
        "  * **4.1: Split the CIFAR-10 dataset** (1 point)\n",
        "  * **4.2: Identify and Correct Errors in the CNN Model** (3 points)\n",
        "  * **4.3: Training procedure** (2 points)\n",
        "  * **4.4: Evaluate** (1 point)\n",
        "  * **4.5: Report** (1 point)\n",
        "* **Question 5: Improve the accuracy** (BONUS) *(5 points)*\n",
        "  * **5.1: Custom model** (3 points)\n",
        "  * **5.2: Pretrained Network** (2 points)\n",
        "\n",
        "**TOTAL POINTS ARE 35, MAXIMUM GRADE IS 30**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a8b40d9c",
      "metadata": {
        "id": "a8b40d9c"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    %pip install -qqq numpy scipy matplotlib pandas scikit-learn seaborn tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3960c7ef",
      "metadata": {
        "id": "3960c7ef"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np # imports a fast numerical programming library\n",
        "import matplotlib.pyplot as plt # sets up plotting under plt\n",
        "import pandas as pd # lets us handle data as dataframes\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# sets up pandas table display\n",
        "pd.set_option('display.width', 500)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.notebook_repr_html', True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c56914",
      "metadata": {
        "id": "35c56914"
      },
      "source": [
        "**Notation:**\n",
        "\n",
        "- $x^i$ is the $i^{th}$ feature vector\n",
        "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
        "- $m$ is the number of training examples\n",
        "- $n$ is the number of features\n",
        "\n",
        "**Let's start by setting up our Python environment and importing the required libraries:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306465a9",
      "metadata": {
        "id": "306465a9",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## 1: **Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba37beb8",
      "metadata": {
        "id": "ba37beb8"
      },
      "source": [
        "### **1.1: Log-likelihood and Gradient Ascent Rule**\n",
        "\n",
        "Write the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n",
        "\n",
        "Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n",
        "\n",
        "Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663c4718",
      "metadata": {
        "id": "663c4718"
      },
      "source": [
        "-------------------------------------------------------\n",
        "\n",
        "**WRITE YOUR EQUATIONS HERE**\n",
        "\n",
        "- **Likelihood**:\n",
        "\\begin{align}\n",
        "L(\\theta) &=\\prod_{i=1}^m P(y_i \\mid x_i; \\theta) = \\prod_{i=1}^m \\left[ h_\\theta(x_i)^{y_i} (1 - h_\\theta(x_i))^{1 - y_i} \\right].\n",
        "\\end{align}\n",
        "\n",
        "- **Log-Likelihood**:\n",
        "\n",
        "\\begin{align}\n",
        "l(\\theta) &=\\log L(\\theta) = \\log \\prod_{i=1}^m \\left[ h_\\theta(x_i)^{y_i} (1 - h_\\theta(x_i))^{1 - y_i} \\right] = \\sum_{i=1}^m \\left[ y_i \\log h_\\theta(x_i) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]\n",
        "\\end{align}\n",
        "\n",
        "- **Gradient of log-likelihood** (slide 5 p. 20):\n",
        "\\begin{align}\n",
        "\\frac{\\delta l(\\theta)}{\\delta \\theta_j} &= \\sum_{i=1}^m (y_i - h_\\theta(x_i)) x_{ij}\n",
        "\\end{align}\n",
        "\n",
        "- **Gradient update equation**:\n",
        "For  $j=0,...,n$:\n",
        "\\begin{equation}\n",
        "\\theta_j =\\theta_j  + \\alpha \\sum_{i=1}^m (y_i - h_\\theta(x_i)) x_{ij}\n",
        "\\end{equation}\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69909e04",
      "metadata": {
        "id": "69909e04"
      },
      "source": [
        "### **1.2: Logistic regression with Gradient Ascent**\n",
        "\n",
        "Define the sigmoid function `sigmoid`, then define the `LogisticRegression` class with the relative methods necessary to make predictions on an input, compute the log-likelihood and update its parameters.\n",
        "Then define a function that takes in input such $X$, $y$ and the predictions $\\hat{y} = g(\\theta^{T}x)$ and computes the gradient of the log-likelihood.\n",
        "Finally implement a function that takes in input such class and performs the training loop with the specified hyperparameters.\n",
        "\n",
        "Translate the equations you wrote above in code to learn the logistic regression parameters, $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.\n",
        "\n",
        "*Hint: even though by definition log likelihood and gradient ascent are defined by summations, for numerical stability it is advised to use the mean operation.*\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "**Fill in the code in `libs/models/logisic_regression.py`, `libs/math.py/sigmoid()` and `libs/optim.py`**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4829929",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "e4829929",
        "outputId": "34d13c0c-ac10-4bd0-c69b-b983fe5a2100"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fit\n",
            "File \u001b[1;32mc:\\Users\\vikto\\Desktop\\wf\\FDS\\Homework02\\libs\\models\\__init__.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogistic_regression_penalized\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegressionPenalized\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultinomial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoftmaxClassifier\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_cnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomCNN\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoor_cnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PoorPerformingCNN\n",
            "File \u001b[1;32mc:\\Users\\vikto\\Desktop\\wf\\FDS\\Homework02\\libs\\models\\custom_cnn.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "from libs.models import LogisticRegression\n",
        "from libs.optim import fit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ca378a",
      "metadata": {
        "id": "c2ca378a"
      },
      "source": [
        "**Check your grad_l implementation:**\n",
        "\n",
        "`LogisticRegression.log_likelihood` applied to some random vectors should provide a value for `output_test` close to the `target_value` (defined below).\n",
        "In other words, `error_test` should be close to 0.\n",
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df146f9",
      "metadata": {
        "id": "4df146f9"
      },
      "outputs": [],
      "source": [
        "target_value = -1\n",
        "np.random.seed(1)\n",
        "output_test = LogisticRegression.likelihood(np.random.random(100), np.random.randint(0, 2, 100))\n",
        "error_test = np.abs(output_test - target_value)\n",
        "print(\"Error: \", error_test)\n",
        "assert error_test < 0.2, \"The output is not correct\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f876a9e2",
      "metadata": {
        "id": "f876a9e2"
      },
      "source": [
        "#### Preprocessing\n",
        "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=800/>\n",
        "\n",
        "Now you'll load a dataset of penguins data. The dataset contains three species of penguins (Adelie, Gentoo and Chinstrap). Your goal will be to classify a penguin species based on their bill's length and body mass. First we'll load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae9d063",
      "metadata": {
        "id": "eae9d063"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"assets/train.csv\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ee1a83",
      "metadata": {
        "id": "07ee1a83"
      },
      "source": [
        "We want to train a classifier capable of understanding the difference between Adelie and Gentoo solely based on their bill's length and body mass. Thus in order to preprocess the data we:\n",
        "1. Drop all the items with null data.\n",
        "2. Remove the third species (Chinstrap) from the dataset.\n",
        "3. Select the features we're interested in (`bill_length`, `body_mass`).\n",
        "4. Select the label data and encode it in the values 0 and 1.\n",
        "\n",
        "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bda351f",
      "metadata": {
        "id": "0bda351f"
      },
      "outputs": [],
      "source": [
        "data.dropna(inplace=True)\n",
        "data = data[data[\"species\"] != \"Chinstrap\"]\n",
        "X = data[[\"bill_length\", \"body_mass\"]]\n",
        "y = data[\"species\"].map({\"Adelie\": 0, \"Gentoo\": 1}).values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3cb402",
      "metadata": {
        "id": "3b3cb402"
      },
      "source": [
        "It is recommended to normalize data when using machine learning techniques, so now normalize $X$ to have $\\mu=0, \\sigma=1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b446b8f0",
      "metadata": {
        "id": "b446b8f0"
      },
      "outputs": [],
      "source": [
        "X = (X - X.mean()) / X.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057099fb",
      "metadata": {
        "id": "057099fb"
      },
      "source": [
        "We add a column of 1's to $X$ to take into account the intercept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118726ed",
      "metadata": {
        "id": "118726ed"
      },
      "outputs": [],
      "source": [
        "X[\"bias\"] = 1\n",
        "# Reordering columns to have the bias term first (convention)\n",
        "X = X[[\"bias\", \"bill_length\", \"body_mass\"]]\n",
        "X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c6c591",
      "metadata": {
        "id": "d4c6c591"
      },
      "source": [
        "#### Training\n",
        "Now you'll use the class defined above to train a logistic regression model on classifying a group of penguins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d70727f",
      "metadata": {
        "id": "3d70727f"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = LogisticRegression(num_features=X.shape[1])\n",
        "\n",
        "# Run Gradient Ascent method\n",
        "n_iter = 50\n",
        "log_l_history, _ = fit(model, X, y, lr=0.5, num_steps=n_iter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589ba43d",
      "metadata": {
        "id": "589ba43d"
      },
      "source": [
        "Let's plot the log likelihood over different iterations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53b2578",
      "metadata": {
        "id": "d53b2578"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(log_l_history)), log_l_history, \"b\")\n",
        "plt.ylabel(\"log-likelihood(Theta)\")\n",
        "plt.xlabel(\"Iterations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f333bcd8",
      "metadata": {
        "id": "f333bcd8"
      },
      "source": [
        "Plot the data and the decision boundary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c2e256",
      "metadata": {
        "id": "24c2e256"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.scatterplot(data=X, x=\"bill_length\", y=\"body_mass\", hue=data[\"species\"])\n",
        "\n",
        "x_range = np.linspace(X['bill_length'].min(), X['bill_length'].max(), 100)\n",
        "theta_final = model.parameters\n",
        "y_range = -(theta_final[0] + theta_final[1] * x_range) / theta_final[2]\n",
        "plt.plot(x_range, y_range, c=\"red\")\n",
        "\n",
        "plt.xlim(X['bill_length'].min() - 0.2, X['bill_length'].max() + 0.2)\n",
        "plt.ylim(X['body_mass'].min() - 0.2, X['body_mass'].max() + 0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df68aa89",
      "metadata": {
        "id": "df68aa89"
      },
      "outputs": [],
      "source": [
        "accuracy = ((model.predict(X) > 0.5) == y).mean()\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "assert accuracy > 0.6, \"The accuracy is too low\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfad244e",
      "metadata": {
        "id": "dfad244e"
      },
      "source": [
        "### **1.3: Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9422a9aa",
      "metadata": {
        "id": "9422a9aa"
      },
      "source": [
        "1. Are we looking for a local minimum or a local maximum using the gradient ascent rule?\n",
        "2. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?\n",
        "3. Let's deeply analyze how the learning rate $\\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results?\n",
        "4. What happens if you do not normalize the data? Try to run the algorithm without normalizing the data and see what happens. Why do you think this happens?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87caf56",
      "metadata": {
        "id": "e87caf56"
      },
      "source": [
        "-------------------------------------------------------\n",
        "\n",
        "\n",
        "**WRITE YOUR ANSWER HERE:**\n",
        "\n",
        "1. The objective function is maximized with the gradient ascent rule. What we are looking for is a local maximum.\n",
        "Gradient ascent increases the function value. It updates parameters toward the positive slope of the gradient.\n",
        "2. Yes, gradient descent could have been used instead of gradient ascent by minimizing the negative log-likelihood $-\\ell(\\theta)$ instead of maximizing $\\ell(\\theta)$. Both approaches work because logistic regression's log-likelihood is a **concave function**, and minimizing $-\\ell(\\theta)$ leads to the same optimal parameters as maximizing $\\ell(\\theta)$. However, gradient ascent is more intuitive for this problem because the goal is to maximize the likelihood directly.\n",
        "The log-likelihood $\\ell(\\theta)$ is a **monotonic increasing function** of the predicted probabilities $h_\\theta(x)$. This implies that as $h_\\theta(x)$ approaches the true labels $y$, the log-likelihood increases, making it intuitive to directly maximize $\\ell(\\theta)$. Therefore, gradient ascent is naturally aligned with the goal of improving the model's predictions by increasing $\\ell(\\theta)$.\n",
        "3.The learning rate $\\alpha$ and the number of iterations significantly influence the convergence of gradient ascent. A larger $\\alpha$ accelerates convergence but risks overshooting or oscillating if too large, while a smaller $\\alpha$ ensures stability but requires more iterations. The number of iterations determines how long the algorithm runs, affecting whether it fully converges to the optimal parameters. These parameters affect the decision boundary because they directly impact the convergence of $\\theta$, which defines the boundary $\\theta^T x + b = 0$. If $\\theta$ does not converge, the boundary may misclassify data.\n",
        "For example, when $\\alpha = 100$, the algorithm experiences instability in the computation of the sigmoid function and log-likelihood, leading to erratic behavior in the graph and unreliable results. Reducing $\\alpha$ to 10 resolves this instability, producing a stable log-likelihood curve that grows quickly and slightly improves accuracy. Further reducing $\\alpha$ to 1 or 0.5 results in slower growth of the log-likelihood, but the accuracy remains the same as with $\\alpha = 10$. However, when $\\alpha = 0.001$, the log-likelihood curve appears as a straight line, indicating very slow learning, and the accuracy is significantly reduced because the parameters fail to converge effectively.\n",
        "Screenshots of the graphs for these cases are provided, illustrating how the learning rate impacts the stability of the log-likelihood and the model’s ability to learn effectively. This demonstrates the importance of selecting a balanced $\\alpha$, such as 0.5 or 1, to ensure both stability and efficient convergence.\n",
        "4.If you do not normalize the data, features with larger scales dominate the learning process, causing non-uniform updates to the model parameters. This leads to slower or unstable convergence, as gradient updates are disproportionately affected by some features. The decision boundary becomes distorted and the model misclassifies the data. Furthermore, numerical instability occurs, particularly in the sigmoid function, causing saturation and making learning inefficient. Data normalization ensures balanced feature contributions, stable convergence, and better performance.\n",
        "5. *(feel free to add here screenshots or new code cells if needed)*\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\alpha = 100$"
      ],
      "metadata": {
        "id": "8N0vqNswUDy3"
      },
      "id": "8N0vqNswUDy3"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(filename='Homework02/libs/alpha100log.png'))\n",
        "display(Image(filename='Homework02/libs/alpha100.png'))"
      ],
      "metadata": {
        "id": "KePAzsBVUGyv"
      },
      "id": "KePAzsBVUGyv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\alpha = 10$"
      ],
      "metadata": {
        "id": "s3MqWgU9UH5r"
      },
      "id": "s3MqWgU9UH5r"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(filename='Homework02/libs/alpha10log.png'))\n",
        "display(Image(filename='Homework02/libs/alpha10.png'))"
      ],
      "metadata": {
        "id": "ErNXySpaULTp"
      },
      "id": "ErNXySpaULTp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\alpha = 1$"
      ],
      "metadata": {
        "id": "ntweDOtjUOtb"
      },
      "id": "ntweDOtjUOtb"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(filename='Homework02/libs/alpha1log.png'))\n",
        "display(Image(filename='Homework02/libs/alpha1.png'))"
      ],
      "metadata": {
        "id": "v2yJrG9vUUyW"
      },
      "id": "v2yJrG9vUUyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\alpha = 0.5$"
      ],
      "metadata": {
        "id": "J7Z5DIDOUZ-q"
      },
      "id": "J7Z5DIDOUZ-q"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(filename='Homework02/libs/alpha0.5log.png'))\n",
        "display(Image(filename='Homework02/libs/alpha0.5.png'))"
      ],
      "metadata": {
        "id": "c6DphlAJUcb4"
      },
      "id": "c6DphlAJUcb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\alpha = 0.001$"
      ],
      "metadata": {
        "id": "TnppxoP1Ukin"
      },
      "id": "TnppxoP1Ukin"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(filename='Homework02/libs/alpha0.001log.png'))\n",
        "display(Image(filename='Homework02/libs/alpha0.001.png))"
      ],
      "metadata": {
        "id": "_E7sLdezUo9X"
      },
      "id": "_E7sLdezUo9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e71ad46b",
      "metadata": {
        "id": "e71ad46b",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## 2: **Polynomial Expansion**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "022266f7",
      "metadata": {
        "id": "022266f7",
        "tags": []
      },
      "source": [
        "### **2.1: Polynomial features for logistic regression**\n",
        "\n",
        "Define new features e.g., of 2nd and 3rd degree, and learn a logistic regression classifier by using the new features and the gradient ascent optimization algorithm defined in Question 1.\n",
        "\n",
        "In particular, consider a polynomial boundary with equation:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2\n",
        "\\end{equation}\n",
        "\n",
        "Therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n",
        "\n",
        "Create new arrays by stacking $x$ and the new 7 features (in the order $x_1x_1, x_2x_2, x_1x_2, x_1x_1x_1, x_2x_2x_2, x_1x_1x_2, x_1x_2x_2$).\n",
        "In particular create `x_new_quad` by additionally stacking $x$ with the quadratic features, and `x_new_cubic` by additionally stacking $x$ with the quadratic and the cubic features.\n",
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e200aed3",
      "metadata": {
        "id": "e200aed3",
        "outputId": "9461aef3-372b-4e55-8433-13908ace5a6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(\n",
        "    n_samples=700,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    n_clusters_per_class=1,\n",
        "    class_sep=.3,\n",
        "    random_state=89,\n",
        ")\n",
        "X = np.hstack([np.ones_like(X[:, [0]]), X])\n",
        "X, X_val, y, y_val = train_test_split(X, y, test_size=200, random_state=42)\n",
        "\n",
        "sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18dc0176",
      "metadata": {
        "id": "18dc0176"
      },
      "outputs": [],
      "source": [
        "def get_polynomial(X, degree):\n",
        "    \"\"\"\n",
        "    Given an initial set of features, this function computes the polynomial features up to the given degree.\n",
        "\n",
        "    Args:\n",
        "        X: the initial features matrix of shape (n_samples, 3) where the first column is the bias term\n",
        "        degree: the degree of the polynomial\n",
        "\n",
        "    Returns:YOUR\n",
        "        X: the final polynomial features\n",
        "    \"\"\"\n",
        "    if degree < 2:\n",
        "        return X\n",
        "\n",
        "    features = np.ones(X.shape[0])\n",
        "\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    # Extract x1 and x2\n",
        "    x1 = X[:, 1]\n",
        "    x2 = X[:, 2]\n",
        "\n",
        "    # Add original features x1 and x2\n",
        "    features = np.column_stack((features, x1, x2))\n",
        "\n",
        "    # Quadratic terms\n",
        "    if degree >= 2:\n",
        "      x1x1 = x1 ** 2\n",
        "      x2x2 = x2 ** 2\n",
        "      x1x2 = x1 * x2\n",
        "\n",
        "      features = np.column_stack((features, x1x1, x2x2, x1x2))\n",
        "\n",
        "    # Cubic terms\n",
        "    if degree >= 3:\n",
        "      x1x1x1 = x1 ** 3\n",
        "      x2x2x2 = x2 ** 3\n",
        "      x1x1x2 = x1 ** 2 * x2\n",
        "      x1x2x2 = x1 * x2 ** 2\n",
        "\n",
        "      features = np.column_stack((features, x1x1x1, x2x2x2, x1x1x2, x1x2x2))\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aca7ebf",
      "metadata": {
        "id": "6aca7ebf"
      },
      "source": [
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611a2aee",
      "metadata": {
        "id": "611a2aee",
        "outputId": "133af0fb-b530-4e47-a59e-91731d45a967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 6) (500, 10)\n"
          ]
        }
      ],
      "source": [
        "x_new_quad = get_polynomial(X, degree=2)\n",
        "x_new_cubic = get_polynomial(X, degree=3)\n",
        "print(x_new_quad.shape, x_new_cubic.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a43fe0",
      "metadata": {
        "id": "57a43fe0"
      },
      "source": [
        "Now use the gradient ascent optimization algorithm to learn the models by maximizing the log-likelihood, both for the case of `x_new_quad` and `x_new_cubic`.\n",
        "\n",
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea40537",
      "metadata": {
        "id": "4ea40537",
        "outputId": "9f25e03c-16a2-4377-8488-3e5ce1b25584"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "LogisticRegression.__init__() got an unexpected keyword argument 'num_features'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model_lin \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m log_l_history,_ \u001b[38;5;241m=\u001b[39m fit(model_lin, X, y, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, num_steps\u001b[38;5;241m=\u001b[39mn_iter)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize model, in case of quadratic features\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: LogisticRegression.__init__() got an unexpected keyword argument 'num_features'"
          ]
        }
      ],
      "source": [
        "n_iter = 50\n",
        "model_lin = LogisticRegression(num_features=X.shape[1])\n",
        "log_l_history,_ = fit(model_lin, X, y, lr=0.5, num_steps=n_iter)\n",
        "\n",
        "# Initialize model, in case of quadratic features\n",
        "model_quad = LogisticRegression(num_features=x_new_quad.shape[1])\n",
        "log_l_history_quad,_ = fit(model_quad, x_new_quad, y, lr=0.5, num_steps=n_iter)\n",
        "\n",
        "# Initialize model, in case of quadratic and cubic features\n",
        "model_cubic = LogisticRegression(num_features=x_new_cubic.shape[1])\n",
        "log_l_history_cubic,_ = fit(model_cubic, x_new_cubic, y, lr=0.5, num_steps=n_iter)\n",
        "\n",
        "log_l = np.stack([log_l_history, log_l_history_quad, log_l_history_cubic])\n",
        "\n",
        "log_l_df = pd.DataFrame(log_l.T, columns=[\"Linear\", \"Quadratic\", \"Cubic\"])\n",
        "sns.lineplot(data=log_l_df, markers=True).set(\n",
        "    xlabel=\"Iterations\", ylabel=\"Log Likelihood\", title=\"Log Likelihood History for Different Models\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b60667ee",
      "metadata": {
        "id": "b60667ee"
      },
      "source": [
        "### **2.2: Plot the computed non-linear boundary**\n",
        "\n",
        "First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcebc714",
      "metadata": {
        "id": "dcebc714"
      },
      "outputs": [],
      "source": [
        "def boundary_function(x1_vec, x2_vec, theta_final, degree):\n",
        "    \"\"\"\n",
        "    This function computes the boundary function for the given theta_final and degree.\n",
        "\n",
        "    Args:\n",
        "        x1_vec: the x1 vector\n",
        "        x2_vec: the x2 vector\n",
        "        theta_final: the final theta\n",
        "        degree: the degree of the polynomial\n",
        "\n",
        "    Returns:\n",
        "        x1_vec: the x1 vector\n",
        "        x2_vec: the x2 vector\n",
        "        f: the boundary function\n",
        "    \"\"\"\n",
        "\n",
        "    x1_vec, x2_vec = np.meshgrid(x1_vec, x2_vec)\n",
        "\n",
        "    #####################################################\n",
        "    ##                 YOUR CODE HERE                  ##\n",
        "    #####################################################\n",
        "\n",
        "    # Flatten the meshgrid for feature calculation\n",
        "    x1_flat = x1_vec.ravel()\n",
        "    x2_flat = x2_vec.ravel()\n",
        "\n",
        "    # Bias term\n",
        "    features = np.ones((x1_flat.shape[0], 1))\n",
        "\n",
        "    # Linear terms\n",
        "    features = np.column_stack([features, x1_flat, x2_flat])\n",
        "\n",
        "    # Quadratic terms\n",
        "    if degree >= 2:\n",
        "        features = np.column_stack([features,\n",
        "                                    x1_flat ** 2,\n",
        "                                    x2_flat ** 2,\n",
        "                                    x1_flat * x2_flat])\n",
        "\n",
        "    # Cubic terms\n",
        "    if degree >= 3:\n",
        "        features = np.column_stack([features,\n",
        "                                    x1_flat ** 3,\n",
        "                                    x2_flat ** 3,\n",
        "                                    (x1_flat ** 2) * x2_flat,\n",
        "                                    x1_flat * (x2_flat ** 2)])\n",
        "\n",
        "    # Compute the boundary function values\n",
        "    f = np.dot(features, theta_final)\n",
        "\n",
        "    return x1_vec, x2_vec, f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b556291",
      "metadata": {
        "id": "0b556291"
      },
      "source": [
        "Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions.\n",
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd0d477",
      "metadata": {
        "id": "6dd0d477"
      },
      "outputs": [],
      "source": [
        "def plot_boundary_function(\n",
        "    X: np.ndarray, y: np.ndarray, theta: np.ndarray, degree: int, n_points: int = 200\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    This function plots the boundary function for the given theta and degree.\n",
        "\n",
        "    Args:\n",
        "        X: the input data\n",
        "        y: the input labels\n",
        "        theta: the final theta\n",
        "        degree: the degree of the polynomial\n",
        "        n_points: the number of points to plot\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    x1_vec = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, n_points)\n",
        "    x2_vec = np.linspace(X[:, 2].min() - 1, X[:, 2].max() + 1, n_points)\n",
        "\n",
        "    x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta, degree=degree)\n",
        "    mesh_shape = int(np.sqrt(f.shape[0]))\n",
        "\n",
        "    sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y, legend=False)\n",
        "    plt.contour(\n",
        "        x1_vec, x2_vec, f.reshape((mesh_shape, mesh_shape)), colors=\"red\", levels=[0]\n",
        "    )\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,3,1)\n",
        "plot_boundary_function(X, y, model_lin.parameters, degree=1)\n",
        "plt.title(\"Decision Boundary for Quadratic Features\")\n",
        "plt.subplot(1,3,2)\n",
        "plot_boundary_function(X, y, model_quad.parameters, degree=2)\n",
        "plt.title(\"Decision Boundary for Quadratic Features\")\n",
        "plt.subplot(1,3,3)\n",
        "plot_boundary_function(X, y, model_cubic.parameters, degree=3)\n",
        "plt.title(\"Decision Boundary for Cubic Features\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12908fe6",
      "metadata": {
        "id": "12908fe6"
      },
      "source": [
        "**Polynomial degree and overfitting**\n",
        "\n",
        "As the polynomial degree increases, the decision boundary becomes more and more complex. This can lead to overfitting, i.e. the model learns the training data too well, and it is not able to generalize to new data. This is a common problem in machine learning, and it is important to be able to detect it.\n",
        "\n",
        "In order to detect overfitting, we can split the dataset into a training set and a test set. The training set is used to learn the model, while the test set is used to evaluate the model performance on new data. If the model performs well on the training set, but it performs poorly on the test set, then we have overfitting.\n",
        "\n",
        "In this exercise, you are asked to plot the training and test accuracy as a function of the polynomial degree. Consider all the polynomial degrees from 1 to 20. For each polynomial degree, learn the model on the training set, and evaluate the accuracy on both the training and the test set. Additionally, visualize the decision boundary for the polynomials that give the **best** and the **worst** test accuracy for $\\texttt{degree} \\geq 2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8c83cf",
      "metadata": {
        "id": "0a8c83cf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def fit_polynomials(X, y, X_test, y_test, degrees, lr, num_steps, architecture = LogisticRegression):\n",
        "    \"\"\"\n",
        "    This function fits a logistic regression model for each degree in the degrees list.\n",
        "    \"\"\"\n",
        "    X = normalize(X)\n",
        "    X_test = normalize(X_test)\n",
        "\n",
        "    thetas = []\n",
        "    accuracy_scores_train, accuracy_scores_test = [], []\n",
        "    for degree in tqdm(degrees):\n",
        "        x_new = get_polynomial(X, degree=degree)\n",
        "\n",
        "        model = architecture(num_features=x_new.shape[1])\n",
        "        fit(model, x_new, y, lr=lr, num_steps=num_steps)\n",
        "\n",
        "        thetas.append(model.parameters)\n",
        "        y_hat_train = model.predict(x_new) > 0.5\n",
        "        accuracy_scores_train.append(accuracy_score(y, y_hat_train))\n",
        "        y_hat_test = model.predict(get_polynomial(X_test, degree=degree)) > 0.5\n",
        "        accuracy_scores_test.append(accuracy_score(y_test, y_hat_test))\n",
        "\n",
        "    return thetas, accuracy_scores_train, accuracy_scores_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28971c0",
      "metadata": {
        "id": "c28971c0"
      },
      "outputs": [],
      "source": [
        "degrees = np.arange(1, 20)\n",
        "np.random.seed(42)\n",
        "thetas, accuracy_scores_train, accuracy_scores_test = fit_polynomials(\n",
        "    X, y, X_val, y_val, degrees=degrees, lr=0.5, num_steps=500, architecture=LogisticRegression\n",
        ")\n",
        "sns.lineplot(x=degrees, y=accuracy_scores_train, label=\"Train\")\n",
        "sns.lineplot(x=degrees, y=accuracy_scores_test,  label=\"Test\")\n",
        "plt.xlabel(\"Degree\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.xticks(degrees)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c0052d",
      "metadata": {
        "id": "c8c0052d"
      },
      "source": [
        "Plot the best and the worst decision boundaries for $\\texttt{degree} \\geq 2$.\n",
        "\n",
        "--------------------------------------------\n",
        "**Write your code below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c483b33",
      "metadata": {
        "id": "7c483b33"
      },
      "outputs": [],
      "source": [
        "# Plot worst model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the indices of the worst test accuracy for degree >= 2\n",
        "worst_degree_index = np.argmin(accuracy_scores_test[1:]) + 1\n",
        "\n",
        "worst_degree = degrees[worst_degree_index]\n",
        "\n",
        "print(f\"Worst degree: {worst_degree} with test accuracy: {accuracy_scores_test[worst_degree_index]:.4f}\")\n",
        "\n",
        "# Plot the decision boundary for the worst degree\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Worst Degree\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_boundary_function(\n",
        "    X=X,\n",
        "    y=y,\n",
        "    theta=thetas[worst_degree_index],\n",
        "    degree=worst_degree\n",
        ")\n",
        "plt.title(f\"Decision Boundary for Worst Degree = {worst_degree}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa71ea4",
      "metadata": {
        "id": "2aa71ea4"
      },
      "outputs": [],
      "source": [
        "# Plot best model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the indices of the best test accuracy for degree >= 2\n",
        "best_degree_index = np.argmax(accuracy_scores_test[1:]) + 1\n",
        "\n",
        "best_degree = degrees[best_degree_index]\n",
        "\n",
        "print(f\"Best degree: {best_degree} with test accuracy: {accuracy_scores_test[best_degree_index]:.4f}\")\n",
        "\n",
        "# Plot the decision boundary for the best degree\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Best Degree\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_boundary_function(\n",
        "    X=X,\n",
        "    y=y,\n",
        "    theta=thetas[best_degree_index],\n",
        "    degree=best_degree\n",
        ")\n",
        "plt.title(f\"Decision Boundary for Best Degree = {best_degree}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6eae9e4",
      "metadata": {
        "id": "e6eae9e4"
      },
      "source": [
        "#### **Report**\n",
        "Write now your considerations. Discuss in particular:\n",
        "1. Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?\n",
        "2. Look at the plot of the training and test accuracy as a function of the polynomial degree. What can you say about the differences between the training and test accuracy? What can you say about the differences between the best and the worst test accuracy? In general, is it desirable to have a very complex decision boundary, i.e. a very high degree of the polynomial? Discuss and motivate your answer.\n",
        "3. In general what are some properties of the dataset that makes it more prone to overfitting? Discuss their impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08abefa0",
      "metadata": {
        "id": "08abefa0"
      },
      "source": [
        "-------------------------------------------------------\n",
        "\n",
        "\n",
        "**WRITE YOUR ANSWER HERE:**\n",
        "\n",
        "1. The differences between linear, quadratic and cubic decision boundaries are that:\n",
        "- **Linear boundary** is is represented by a straight line that separates the two classes, but doing like this it can’t capture the non-linear relationships between them. So, this is an example of underfitting, because the model is too simple to represent the data structure.\n",
        "- **Quadratic boundary**, respect to the linear boundary, introduces a second-degree polynomial, we can see that because there’s a curve. This curve helps to separate the two classes in a better way, because it captures the data’s non linear patterns. In this case there’s no presence of underfitting.\n",
        "- **Cubic boundary** includes a third-degree polynomial terms, resulting therefore even more flexible than the Quadratic boundary. However, this can lead to overfitting, meaning that the model begins to capture noises of the training data, which are irrelevant, rather than the true pattern.\n",
        "\n",
        "Increasing the polynomial degree, it’s easy to see that the model improves in performance, but, as it shows us the Cubic boundary, increases in degree don’t lead always to better generalization, sometimes it could lead to overfitting.\n",
        "\n",
        "Increasing more and more the degree we can’t incur in underfitting because it typically occurs with simple models (like Linear loundaries). So, increasing the polynomial degree decreases the possibility of incurring in underfitting (as it doesn’t occur with higher degrees), but increasing too much the degree could lead to overfitting.\n",
        "\n",
        "\n",
        "2. As regards the differences between the training and test accuracy, we can say that while the training accuracy increases as the polynomial degree increases, the test accuracy, after reaching a certain height (degree 2), starts to decrease as the degree increases, becoming stable (after degree 3). This, as said in point 1., reflects the model's overfitting to the training data due to the fact that the degree is too high.\n",
        "\n",
        "About the differences between the best and the worst test accuracy, we can say that the best test accuracy occurs when the degree is 2 and, as we said before, the decision boundary manages to capture the non-linear structure of the data, without incurring overfitting.\n",
        "The worst degree test accuracy occurs when the degree is 3 (or greater than 3) and it occurs due to the fact that the model captures noises in the training data (instead than true patterns), incurring overfitting.  The best boundary is more generalizable than the worst boundary.\n",
        "\n",
        "About the desirability of a very complex decision boundary, we can say that it’s not very desirable, because it increases the risk of overfitting. Even if a model achieves a good training accuracy, it will probably have a bad test accuracy, meaning that the model is not very generalizable.\n",
        "A model with a very high degree of the polynomial tends to have bad test accuracy, this happens because it captures noises that are irrelevant, failing to capture the true structure od the data.\n",
        "\n",
        "To conclude, as this example shows us, to capture the data's structure and generalize well it's not mandatory to have a degree that high, because in this case a degree = 2 works well.\n",
        "\n",
        "3. As said in the previous points, the overfitting occurs when a model captures noises and irrelevant details in the data.\n",
        "Some properties of the datasets that makes them more prone to overfitting are for example:\n",
        "\n",
        "- **Polynomial degree of the model**, as said in the previous points. We know that as the degree increases, the decision boundary becomes more flexible, but if the degree is too high, this can lead to overfitting, and the model begins to capture noises, reducing its ability to generalize in a good way.\n",
        "- **Small dataset size**, because it increases the likelihood that the model memorizes the training data instead of generalizing new data.\n",
        "- **High dimensionality of the dataset**, because data become sparse and the model becomes to flexible, and this, as the high polynomial degree issue, can lead to overfitting.\n",
        "- **Highly complex data patterns**, because also in this case the model can capture other things instead of the true patterns of the data.\n",
        "\n",
        "\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f456b6e",
      "metadata": {
        "id": "1f456b6e"
      },
      "source": [
        "### **2.4: Weight Penalization**\n",
        "\n",
        "Look at how complicated the decision boundaries become as you increase the degree. Can we improve this and prevent overfitting?\n",
        "When dealing with overfitting one frequent solution is to use a weigth penalization technique like L2 or L1 penalization.\n",
        "\n",
        "In our case we'll use L2 regularization. In this way the regularized likelihood will be:\n",
        "$$\n",
        "\\texttt{Likelihood}_{reg}(\\theta) = \\texttt{Likelihood}(\\theta) - \\frac{\\lambda}{2n} \\sum^n_i \\theta_i^2\n",
        "$$\n",
        "Thus we can derive the update rule as:\n",
        "\\begin{equation}\n",
        "\\theta_j:= \\theta_j + \\alpha( \\frac{\\partial l(\\theta_j)}{\\partial \\theta_j} -  \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{\\lambda}{2} \\theta_j^2 \\right ) )\n",
        "\\end{equation}\n",
        "\n",
        "Calculating the second term of the update rule it's just a matter of analytically solving a simple gradient, do it, and then implement it by extending the `LogisticRegression` class:\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "**Fill in the code in `libs/models/logisic_regression_penalized.py`**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7cc62c",
      "metadata": {
        "id": "fb7cc62c"
      },
      "outputs": [],
      "source": [
        "from libs.models import LogisticRegressionPenalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19371cf3",
      "metadata": {
        "id": "19371cf3"
      },
      "outputs": [],
      "source": [
        "X,y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=100,\n",
        "    n_informative=50,\n",
        "    n_redundant=25,\n",
        "    n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "np.random.seed(42)\n",
        "\n",
        "lr = LogisticRegression(X.shape[1])\n",
        "likelihood_history, val_loss_history = fit(lr, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
        "\n",
        "penalized_lt = LogisticRegressionPenalized(X.shape[1], 2)\n",
        "pen_history, pen_val_history = fit(penalized_lt, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(-likelihood_history[2:], label=\"Train\", color=\"violet\")\n",
        "plt.plot(val_loss_history[2:], label=\"Test\", color='teal')\n",
        "plt.plot(-pen_history[2:], label=\"Train - penalized\", color=\"violet\", linestyle=\"--\")\n",
        "plt.plot(pen_val_history[2:], label=\"Test - penalized\", color=\"teal\", linestyle=\"--\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Non-penalized\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bee8a3",
      "metadata": {
        "id": "58bee8a3"
      },
      "source": [
        "Now, evaluate the Penalized Logistic Regression for each value of $\\lambda \\in [0,3]$ and find the one that performs the best:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb84e3bf",
      "metadata": {
        "id": "cb84e3bf"
      },
      "outputs": [],
      "source": [
        "lambdas = np.arange(0, 3, 0.1)\n",
        "losses = []\n",
        "\n",
        "for lambda_ in lambdas:\n",
        "    ##############################################\n",
        "    ###         COMPLETE THIS FOR-LOOP         ###\n",
        "    ##############################################\n",
        "\n",
        "    # Inizialize Penalized Logistic Regression with the current lambda\n",
        "    model = LogisticRegressionPenalized(num_features=X.shape[1], lambda_=lambda_)\n",
        "\n",
        "    # Train the model\n",
        "    _, val_loss_history = fit(model, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n",
        "\n",
        "    # Append the last validation loss to the losses list\n",
        "    final_val_loss = val_loss_history[-1]\n",
        "    losses.append(final_val_loss)\n",
        "\n",
        "if len(losses) > 0:\n",
        "    sns.lineplot(x=lambdas, y=losses, label=\"Validation Loss\").set(\n",
        "        xlabel=\"Lambda\", ylabel=\"Loss\", title=\"Validation Loss vs Lambda\"\n",
        "    )\n",
        "    print(f\"Best lambda: {lambdas[np.argmin(losses)]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9a196b",
      "metadata": {
        "id": "8a9a196b"
      },
      "source": [
        "#### Report\n",
        "Write now your considerations. In particular:\n",
        "1. What happens when we use a non-penalized logistic regression?\n",
        "2. Observe the plot of the Train and Validation losses in the penalized vs non penalized case. In which case is the Train loss better? Can you explain why?\n",
        "3. What is the convergence rate? How is it influenced by the penalization?\n",
        "\n",
        "-------------------------------------------------------\n",
        "\n",
        "\n",
        "**WRITE YOUR ANSWER HERE:**\n",
        "\n",
        "1. If we use a **non-penalized logistic regression**, this can lead to the absence of regularization, which can results in overfitting. The model creates complex decision boundaries that fit the noise in the training data.\n",
        "This leads to low training loss but poor generalization to the validation or test set, resulting in higher validation loss.\n",
        "Another thing that can happens is that the weights can grow unbounded as the optimization algorithm prioritizes minimizing the training error. This could lead make the model highly sensitive to small changes in input features. This behavior often results in a high variance model, where the model performs well on the training data but poorly on unseen data, overfitting also in this case. Another thing to say is that the decision boundaries produced by non-penalized logistic regression tend to be unnecessarily complex and wavy.\n",
        "In conclusion, non-penalized logistic regression is effective at minimizing training loss, but it is also highly prone to overfitting and poor generalization.\n",
        "\n",
        "2. One thing visible in the plot is that the non-penalized model has a lower training loss if we compare that to the penalized model (the pink line is the non-penalized training loss, and it’s always under the dashed pink line, which is the penalized training loss).\n",
        "This happens because the non-penalized model adjusts its weights without any constraints, allowing it to fit the training data in a good way. This leads to the minimizing of the training loss in a more effectively way.\n",
        "On the other hand, the penalized model applies L2 regularization, which discourages large weight magnitudes by introducing a penalty term to the loss function. This prevents the model from overfitting to the training data, this is visible in a higher train loss.\n",
        "\n",
        "3. The convergence rate is the speed at which an algorithm approaches the minimum of the loss function during training. In this case, it’s how quickly the model's parameters stabilize as the loss decreases with successive iterations. Penalization, which is the presence (or absence) of regularization, influences the convergence because a faster convergence rate means that the model requires fewer iterations to reach a point where further updates result in minimal improvements to the loss.\n",
        "Penalization introduces an additional term to the loss function that modifies the optimization landscape, influencing the convergence rate.  \n",
        "Furthermore, penalization stabilizes the optimization process by making weight updates more consistent, improves the condition number of the loss surface and prevents excessive weight growth.\n",
        "So, in conclusion we can say that the penalization impacts convergence by doing all these things.\n",
        "\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7faba031",
      "metadata": {
        "id": "7faba031",
        "tags": []
      },
      "source": [
        "## 3: **Multinomial Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e107612c",
      "metadata": {
        "id": "e107612c"
      },
      "source": [
        "### **3.1: Softmax Regression Model**\n",
        "\n",
        "In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\\in\\{1,...,K\\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K.\n",
        "In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:\n",
        "\n",
        "\\begin{align*}\n",
        "s_i =  X_i \\theta\n",
        "\\end{align*}\n",
        "\n",
        "Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:\n",
        "\n",
        "\\begin{align*}\n",
        "p_{ik} = \\frac{\\exp(X_i \\theta_k)}{\\sum_{j=1}^K(X_i \\theta_j))}\n",
        "\\end{align*}\n",
        "\n",
        "We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*.\n",
        "In the first of this exercise we have to:\n",
        "-    Write the equations of the Cross Entropy loss for the Softmax regression model;\n",
        "-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.\n",
        "\n",
        "#### A bit of notation\n",
        "\n",
        "*  N: is the number of samples\n",
        "*  K: is the number of classes\n",
        "*  X: is the input dataset and it has shape (N, H) where H is the number of features\n",
        "*  y: is the output array with the labels; it has shape (N, 1)\n",
        "*  $\\theta$: is the parameter matrix of the model; it has shape (H, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "753c8304",
      "metadata": {
        "id": "753c8304"
      },
      "source": [
        "--------------------------------------------\n",
        "**Write you equation below this line**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10170a74",
      "metadata": {
        "id": "10170a74"
      },
      "source": [
        "\\begin{align*}\n",
        "L(\\theta) = - \\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\log(p_{ik})\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "Loss(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\log(p_{ik})\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\theta_k} L(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N (y_{ik} - p_{ik}) X_i\n",
        "\\end{align*}\n",
        "\n",
        "# Cross Entropy Loss for Softmax Regression\n",
        "The Cross Entropy loss function measures how well the predicted probabilities match the true labels. The loss function for softmax regression is defined as follows:\n",
        "\n",
        "# Softmax Function\n",
        "The probability \\(p_{ik}\\) for class \\(k\\) for the \\(i\\)-th data sample\n",
        "\n",
        "# Gradient of Cross Entropy Loss with respect to \\(\\theta\\)\n",
        "To perform gradient descent, we need to compute the gradient of the loss function with respect to the model parameters \\(\\theta\\).\n",
        "This gradient indicates how the loss changes with respect to each parameter \\(\\theta_k\\), which is used to update the parameters during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ccc0db",
      "metadata": {
        "id": "60ccc0db"
      },
      "source": [
        "### **3.2: Coding**\n",
        "\n",
        "We are using the CIFAR-10 dataset for this exercise. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It has 50,000 training images and 10,000 test images. The dataset was established by the Canadian Institute For Advanced Research (CIFAR), and it has become a standard benchmark for machine learning algorithms, especially in the area of image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e56f3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "b9e56f3a",
        "outputId": "47cac39e-f029-429b-815b-88563b905b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to assets/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [01:00<00:00, 2.80MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting assets/cifar10/cifar-10-python.tar.gz to assets/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe3fe39f1fd1>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "cifar_dir = \"assets/cifar10\"\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=cifar_dir, train=True, download=True, transform=transform\n",
        ")\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=cifar_dir, train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "def one_hot_encode(y, num_classes=10):\n",
        "    encoded = np.zeros((len(y), num_classes))\n",
        "    for i, val in enumerate(y):\n",
        "        encoded[i, val] = 1\n",
        "    return encoded\n",
        "\n",
        "# Evaluate the accuracy of the predictions\n",
        "def compute_accuracy(predictions, true_labels):\n",
        "    correct_predictions = np.sum(predictions == true_labels)\n",
        "    total_predictions = len(true_labels)\n",
        "    return correct_predictions / total_predictions\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = [img.reshape(-1).numpy() for img, _ in train_data]\n",
        "X_train = np.array(X_train)\n",
        "y_train = [label for _, label in train_data]\n",
        "\n",
        "X_val = [img.reshape(-1).numpy() for img, _ in test_data]\n",
        "X_val = np.array(X_val)\n",
        "y_val = [label for _, label in test_data]\n",
        "\n",
        "\n",
        "# Add bias term to X\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "\n",
        "y_train_onehot = one_hot_encode(y_train)\n",
        "y_test_onehot = one_hot_encode(y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508f30d9",
      "metadata": {
        "id": "508f30d9"
      },
      "source": [
        "*Hint: consider the labels as one-hot vector. This will allow matrix operations (element-wise multiplication and summation).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d63efe6",
      "metadata": {
        "id": "5d63efe6"
      },
      "source": [
        "Now implement a classifier for Multinomial Classification using the `softmax` function. Again, implement it as a class with the methods:\n",
        "- `predict`\n",
        "- `predict_labels`\n",
        "- `likelihood` *(Here you need to implement the Cross Entropy Loss)*\n",
        "- `update_theta`\n",
        "- `compute_gradient` to compute the Jacobian $\\nabla$\n",
        "\n",
        "Note that this this you don't need to reimplement the `fit()` function since the training loop you defined above works also for a Multinomial Classifier, provided that this is structured with the previously mentioned methods.\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "**Fill in the code in `libs/models/multinomial.py` and `libs/math.py/softmax()`**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbca091",
      "metadata": {
        "id": "4bbca091"
      },
      "outputs": [],
      "source": [
        "from libs.models import SoftmaxClassifier\n",
        "from libs.optim import fit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c21abf9",
      "metadata": {
        "id": "8c21abf9"
      },
      "source": [
        "--------------------------------------------\n",
        "\n",
        "**Do not write below this line just run it**\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "*Execution can take around 10 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7353b3df",
      "metadata": {
        "id": "7353b3df"
      },
      "outputs": [],
      "source": [
        "# Apply gradient descent to optimize theta\n",
        "alpha = 0.01\n",
        "iterations = 500\n",
        "H, K = X_train.shape[1], 10  # number of features and number of classes\n",
        "model = SoftmaxClassifier(num_features=H, num_classes=K)\n",
        "loss_history, _ = fit(model, X_train, y_train_onehot, lr=alpha, num_steps=iterations)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = model.predict_labels(X_train)\n",
        "test_predictions = model.predict_labels(X_val)\n",
        "\n",
        "train_accuracy = compute_accuracy(train_predictions, y_train)\n",
        "test_accuracy = compute_accuracy(test_predictions, y_val)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46307d23",
      "metadata": {
        "id": "46307d23"
      },
      "source": [
        "### **3.3: Pipeline**\n",
        "Now you're going to use `scikit-learn` library to build a pipeline of operations to redo everything we've done so far in the homework. First we have loaded the required modules and the penguins dataset.\n",
        "\n",
        "---\n",
        "\n",
        "Then here you'll build the pipeline. We need four items:\n",
        "1. The Numerical Transformer, to handle the preprocessing of numerical columns, by:\n",
        "    - Imputing missing values with their mean\n",
        "    - Enrich the features with a 3-rd degree polynomial expansion\n",
        "    - Scaling of the features to $\\mu=0, \\sigma=1$\n",
        "2. The Categorical Transformer, to handle the preprocessing of categorical values, by:\n",
        "    - Imputing the missing values with the most frequent value\n",
        "    - Encode the features in a one-hot vector.\n",
        "3. The Preprocessor: a ColumnTransformer that distributed the numerical columns to the numerical transformer and the categorical columns to the categorical tranformer.\n",
        "4. The final Pipeline, which contains the preprocessor and the classfier of your choice (in this case `KNeighborsClassifier`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffae09bf",
      "metadata": {
        "id": "ffae09bf"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('assets/train.csv')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(data.drop('species', axis=1), data.species, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91db325f",
      "metadata": {
        "id": "91db325f"
      },
      "outputs": [],
      "source": [
        "numerical_cols = ['bill_length', 'bill_depth', 'flipper_length', 'body_mass']\n",
        "categorical_cols = ['island', 'sex']\n",
        "\n",
        "##############################################\n",
        "###          FILL IN THIS CODE           #####\n",
        "##############################################\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "## Import everything you need here\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "numeric_transformer = make_pipeline(\n",
        "    SimpleImputer(strategy='mean'),  # Impute missing values with mean\n",
        "    PolynomialFeatures(degree=3, include_bias=False), # 3rd-degree polynomial expansion\n",
        "    StandardScaler()  # Scale features to μ=0, σ=1\n",
        ")\n",
        "\n",
        "categoric_transfomer = make_pipeline(\n",
        "    SimpleImputer(strategy='most_frequent'),  # Impute missing values with most frequent value\n",
        "    OneHotEncoder(handle_unknown='ignore')  # One-hot encode categories\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numerical_cols),  # Apply numeric transformer to numerical columns\n",
        "    ('cat', categoric_transfomer, categorical_cols)  # Apply categorical transformer to categorical columns\n",
        "], remainder='drop')\n",
        "\n",
        "pipe = make_pipeline( # Create a pipeline\n",
        "    preprocessor,  # Preprocess data\n",
        "    KNeighborsClassifier()  # Apply K-Nearest Neighbors\n",
        ")\n",
        "\n",
        "if len(pipe.named_steps)>0:\n",
        "    display(pipe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a20cc4b",
      "metadata": {
        "id": "9a20cc4b"
      },
      "source": [
        "Now, we can use this pipeline to preprocess the input data and fit a classifier. Leveraging `scikit-learn`'s pipelines allows you to:\n",
        "- Define the entire chain of operations in a structured way, which is especially useful for cleaning and transforming data.\n",
        "- Separate the definition of operations from their execution, creating a clean and organized workflow.\n",
        "\n",
        "This approach makes it easier to manage complex preprocessing steps while maintaining readability and clarity in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1829845c",
      "metadata": {
        "id": "1829845c"
      },
      "outputs": [],
      "source": [
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37fd86ac",
      "metadata": {
        "id": "37fd86ac"
      },
      "source": [
        "This is nice but can we improve it? In defining the pipeline you certainly used some fixed hyperparameters, for example the number of neighbors or the degree of the polynomial expansion.\n",
        "\n",
        "First, let's look at the list of hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcfd619",
      "metadata": {
        "id": "4dcfd619"
      },
      "outputs": [],
      "source": [
        "hparams = pipe.get_params()\n",
        "for hp, val in hparams.items():\n",
        "    if type(val) not in [int, float, str]:\n",
        "        continue\n",
        "    print(f\"{hp}: {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a99eea0",
      "metadata": {
        "id": "7a99eea0"
      },
      "source": [
        "Some of these hyperparameters are set to their default values, while others are explicitly defined. However, any data scientist knows that hyperparameters should not be arbitrarily chosen; instead, they should be optimized through **Cross-Validation**.\n",
        "\n",
        "We can leverage the compositionality of `scikit-learn` by incorporating the pipeline into a `GridSearchCV` class. This allows you to easily define a grid of parameters to val and automatically perform cross-validation over the combinations.\n",
        "\n",
        "Choose at least 2 values for at least 3 hyperparameters. val their impact on the model and find the best combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3389cc26",
      "metadata": {
        "id": "3389cc26"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid = dict(\n",
        "    columntransformer__num__polynomialfeatures__degree=[2, 3, 4],  # Test polynomial degrees\n",
        "    kneighborsclassifier__n_neighbors=[3, 5, 7],  # Test different numbers of neighbors\n",
        "    kneighborsclassifier__weights=['uniform', 'distance'],  # Test weighting strategies\n",
        "    kneighborsclassifier__metric=['minkowski', 'manhattan'],  # Test distance metrics\n",
        ")\n",
        "\n",
        "#pipe_cv = None\n",
        "# Set up the grid search with cross-validation\n",
        "pipe_cv = GridSearchCV(pipe, param_grid=grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1, return_train_score=True)\n",
        "\n",
        "if pipe_cv is not None:\n",
        "    pipe_cv.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best score: {pipe_cv.best_score_}\")\n",
        "    for hp, val in pipe_cv.best_params_.items():\n",
        "        print(f\"{hp}: {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054a24ba",
      "metadata": {
        "id": "054a24ba"
      },
      "source": [
        "`GridSearchCV` doesn't only find the best combination of hyperparmeters, but it also refits the model with the best hyperparameters it finds. Let's val this new model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76df863",
      "metadata": {
        "id": "d76df863"
      },
      "outputs": [],
      "source": [
        "y_pred = pipe_cv.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "661c4ac8",
      "metadata": {
        "id": "661c4ac8"
      },
      "source": [
        "#### Report\n",
        "1. How many combinations has your gridsearch tried?\n",
        "2. Make a plot with the results of your hyperparameter grid\n",
        "3. Do you notice any trend in the performance of certain hyperparameters?\n",
        "4. Do the classifiers obtain the same accuracy on train and val sets? If not, try to give an explanation.\n",
        "5. With the choice of hyperparameters you made, do you notice any trade-off between accuracy and compute power? Show with a plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be3ce062",
      "metadata": {
        "id": "be3ce062"
      },
      "outputs": [],
      "source": [
        "cv_res = pd.DataFrame(pipe_cv.cv_results_)\n",
        "cv_res.columns = [col.split('__')[-1] for col in cv_res.columns]\n",
        "\n",
        "##############################################\n",
        "###                YOUR CODE HERE         ####\n",
        "##############################################\n",
        "\n",
        "# How many combinations has your gridsearch tried?\n",
        "num_combinations = len(cv_res)\n",
        "print(f\"Grid search tried {num_combinations} combinations.\")\n",
        "\n",
        "# Make a plot with the results of your hyperparameter grid\n",
        "plt.figure(figsize=(10, 6))\n",
        "for weight in cv_res['weights'].unique():\n",
        "    subset = cv_res[cv_res['weights'] == weight]\n",
        "    plt.plot(subset['n_neighbors'], subset['mean_test_score'], label=f'Weights: {weight}')\n",
        "\n",
        "plt.xlabel('Number of Neighbors')\n",
        "plt.ylabel('Mean Test Accuracy')\n",
        "plt.title('Hyperparameter Grid Search Results')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Do you notice any trend in the performance of certain hyperparameters?\n",
        "best_params = pipe_cv.best_params_\n",
        "print(f\"The best hyperparameters are: {best_params}\")\n",
        "print(f\"The best mean accuracy is: {pipe_cv.best_score_:.4f}\")\n",
        "\n",
        "# Do the classifiers obtain the same accuracy on train and validation sets?\n",
        "cv_res['train_val_diff'] = cv_res['mean_train_score'] - cv_res['mean_test_score']\n",
        "print(f\"Train-Validation Differences:\\n{cv_res[['train_val_diff', 'mean_test_score', 'mean_train_score']].head()}\")\n",
        "\n",
        "# With the choice of hyperparameters you made, do you notice any trade-off between accuracy and compute power?\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(cv_res['mean_fit_time'], cv_res['mean_test_score'], c='blue', label='Accuracy vs Fit Time')\n",
        "plt.xlabel('Mean Fit Time (s)')\n",
        "plt.ylabel('Mean Test Accuracy')\n",
        "plt.title('Trade-off Between Accuracy and Compute Power')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6e95b8",
      "metadata": {
        "id": "db6e95b8"
      },
      "source": [
        "--------------------------------------------\n",
        "\n",
        "**1. How many combinations has your grid search tried?**\n",
        "\n",
        "The grid search explored a total of **\\(X\\)** hyperparameter combinations, where \\(X\\) is the product of all possible values for each tested hyperparameter. This includes different values for `polynomialfeatures__degree`, `kneighborsclassifier__n_neighbors`, `kneighborsclassifier__weights`, and `kneighborsclassifier__metric`. Testing such a wide range of combinations allows for an extensive search for optimal hyperparameters that neither underfit nor overfit a model. The number of combinations, however, does affect the computational cost => each combination has to be cross-validated and its model evaluated.\n",
        "\n",
        "**2. Make a plot with the results of your hyperparameter grid.**\n",
        "\n",
        "A plot of the grid search results shows how the test accuracy of the model varies as various combinations of these hyperparameters are tried. This may show, for instance, that with an increase in the number of neighbors (n_neighbors), the trend could be such that accuracy initially rises but then flattens out or decreases due to oversmoothing. As in many weighting schemes, the weights parameter signals the importance of neighbors, often providing insight into whether weighting by distance (distance) is informative compared to uniform weighting (uniform). This kind of insight—when different hyperparameters do and do not influence performance—provides clear guidance.\n",
        "\n",
        "**3. Do you notice any trend in the performance of certain hyperparameters?**\n",
        "\n",
        "The best hyperparameters that resulted from the analysis are `n_neighbors=Y`, `weights='Z'`, and `metric='W'`. These were the hyperparameters for which the maximum cross-validated mean accuracy was **\\(A\\)%**. A functional trend that might be evident could be the fact that high polynomial degrees enrich features, which might contribute to the performance improvement up to some degree when making it too high leads to overfitting. The same holds true for the number of neighbors, `n_neighbors`: smaller values are generally better for small datasets; larger values reduce sensitivity to noise. These trends illustrate a tension between model complexity and generalization.\n",
        "\n",
        "**4. Do the classifiers obtain the same accuracy on train and validation sets?**\n",
        "\n",
        "These differences in training and validation accuracy suggest that the model mostly overfits to some hyperparameters. For example, a high polynomial degree or a small number of neighbors increases the training accuracy but decreases the validation accuracy, which is indicative of overfitting. In contrast, when this difference is small, it means generalization to unseen data goes well. This balance of training and validation performance is what one wants for a robust model and underlines how this tuning of hyperparameters helps to avoid overfitting.\n",
        "\n",
        "**5. With the choice of hyperparameters you made, do you notice any trade-off between accuracy and compute power?**\n",
        "\n",
        "This trade-off is shown in the plot of the relationship between the mean fit time versus the mean test accuracy, where configurations with high polynomial degrees or more neighbors are taking notably longer to compute. In a real-world application, the setting with the best configuration could offer barely marginal improvement on accuracy to afford much more computational cost. This way, the choice of hyperparameter is a trade-off between high accuracy and acceptable computational efficiency for a particular use case and regarding the available resources.\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0a3c8e",
      "metadata": {
        "id": "8a0a3c8e"
      },
      "source": [
        "## **4: Debugging a CNN with Shape Errors**\n",
        "\n",
        "You are provided with a CNN model intended to classify images from the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. However, the model contains shape mismatches between layers due to intentional errors. Your first task is to identify and fix these errors to make the model functional.*testo in corsivo*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d941ac13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d941ac13",
        "outputId": "95cff84b-82ec-4928-a899-c2b85ce99c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776d671f",
      "metadata": {
        "id": "776d671f"
      },
      "source": [
        "### 4.1: Split the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0881a353",
      "metadata": {
        "id": "0881a353"
      },
      "source": [
        "Instructions:\n",
        "\n",
        "* Define the Split Sizes:\n",
        "Calculate the sizes for the training and validation datasets. Allocate ***80% of the training*** data for the training set and *20% for the validation set*.\n",
        "\n",
        "* Split the Dataset:\n",
        "Use `torch.utils.data.random_split` to create the training and validation datasets from the original training dataset.\n",
        "\n",
        "* Create Data Loaders:\n",
        "Create data loaders for the training, validation, and test datasets using torch.utils.data.DataLoader with a ***batch size of 64***. Ensure that the training data is ***shuffled***.\n",
        "\n",
        "* Print the size of each dataset (train, test, val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b82813",
      "metadata": {
        "id": "c5b82813"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the data that we will use\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "#####################################################\n",
        "##              YOUR CODE HERE                     ##\n",
        "#####################################################\n",
        "\n",
        "# Define sizes of train and validation datasets\n",
        "dataset_len = len(full_train_dataset)\n",
        "train_size = int(dataset_len * 0.8)\n",
        "val_size = int(dataset_len - train_size)\n",
        "\n",
        "# Print the size of the each dataset\n",
        "print(f\"Sizes: [train = {train_size}], [test = {len(test_dataset)}], [val = {val_size}]\")\n",
        "\n",
        "# Randomly splite full_train_dataset to the train and validation datasets by size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ba0401",
      "metadata": {
        "id": "96ba0401"
      },
      "source": [
        "### 4.2: Identify and Correct Errors in the CNN Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5806744d",
      "metadata": {
        "id": "5806744d"
      },
      "source": [
        "In this exercise, you will analyze an intentionally incorrect implementation of a Convolutional Neural Network model. Your task is to identify the errors in the `PoorPerformingCNN` class and correct them to ensure the model works properly for the CIFAR-10 dataset.\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "**Fill in the code in `libs/models/poor_cnn.py**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84d190e2",
      "metadata": {
        "id": "84d190e2"
      },
      "outputs": [],
      "source": [
        "from libs.models import PoorPerformingCNN\n",
        "\n",
        "net = PoorPerformingCNN()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bfb7e81",
      "metadata": {
        "id": "5bfb7e81"
      },
      "source": [
        "Loss Function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73233d5",
      "metadata": {
        "id": "f73233d5"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b442d3",
      "metadata": {
        "id": "05b442d3"
      },
      "source": [
        "### 4.3: Training procedure\n",
        "\n",
        "In this exercise, you will complete the training and validation loop of a neural network model. Your task is to compute and store the average training loss, average validation loss, and the corresponding accuracies for each epoch.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "* **Training Phase**:\n",
        "After computing the average training loss (avg_train_loss), you need to calculate the training accuracy based on the model's predictions and append the calculated training accuracy to the train_accuracies list.\n",
        "\n",
        "* **Validation Phase**:\n",
        "After calculating the average validation loss (avg_val_loss), you need to calculate the validation accuracy based on the validation dataset and append the calculated validation accuracy to the val_accuracies list. (the same as befor but for the val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3858bb39",
      "metadata": {
        "id": "3858bb39"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    total_batches = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training Phase\n",
        "    net.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        # Get predictions\n",
        "        _, y_pred = torch.max(outputs, 1)\n",
        "        correct += (y_pred == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    #####################################################\n",
        "    ##              YOUR CODE HERE                     ##\n",
        "    #####################################################\n",
        "\n",
        "    # Calculate average loss for the training epoch\n",
        "    avg_train_loss = running_loss / total_batches\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = correct / total\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    #####################################################\n",
        "    ##              END OF YOUR CODE                   ##\n",
        "    #####################################################\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "    # Validation Phase\n",
        "    net.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_data in val_loader:\n",
        "            val_inputs, val_labels = val_data\n",
        "\n",
        "            # Forward pass\n",
        "            val_outputs = net(val_inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            val_loss = criterion(val_outputs, val_labels)\n",
        "            val_running_loss += val_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "            total += val_labels.size(0)\n",
        "            correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "\n",
        "    #####################################################\n",
        "    ##              YOUR CODE HERE                     ##\n",
        "    #####################################################\n",
        "\n",
        "    # Calculate average loss for the validation epoch\n",
        "    avg_val_loss = val_running_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = correct / total\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    #####################################################\n",
        "    ##              END OF YOUR CODE                   ##\n",
        "    #####################################################\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb35d88",
      "metadata": {
        "id": "6bb35d88"
      },
      "source": [
        "### 4.4: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e248e772",
      "metadata": {
        "id": "e248e772"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #####################################################\n",
        "        ##              YOUR CODE HERE                     ##\n",
        "        #####################################################\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        #####################################################\n",
        "        ##              END OF YOUR CODE                   ##\n",
        "        #####################################################\n",
        "\n",
        "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df9755d",
      "metadata": {
        "id": "2df9755d"
      },
      "source": [
        "### 4.5: Report\n",
        "\n",
        "1. What challenges can class imbalance introduce when training a machine learning model?\n",
        "\n",
        "2. What are some strategies to address class imbalance in your dataset or training process?\n",
        "\n",
        "3. Why might accuracy alone be misleading as a performance measure in the presence of class imbalance\n",
        "\n",
        "4. Is the cifar-10 and imbalanced dataset? plot the number of samples for each classes inside the cifar 10 dataset.\n",
        "\n",
        "-------------------------------------------------------\n",
        "\n",
        "\n",
        "**WRITE YOUR ANSWER HERE:**\n",
        "\n",
        "1. There are few possible cases:\n",
        "  - **Bias**. The trained model predicting the majority class more often, even for inputs belonging to the minority class.\n",
        "  - **High Accuracy, Low Performance**. A model trained on imbalanced data may appear to have high accuracy but may perform poorly on minority classes, which are often the ones of greater interest.\n",
        "  - **Missed Insights**. Data imbalance can result in the loss of important insights and patterns present in the minority class, leading to missed opportunities or critical errors\n",
        "  \n",
        "  `Source`: https://www.isi-web.org/sites/default/files/2024-02/Handling-Data-Imbalance-in-Machine-Learning.pdf\n",
        "2. Addressing class imbalance in machine learning is crucial for developing robust models. There are several strategies to tackle this issue:\n",
        "\n",
        "  - **Data-Level Methods**:\n",
        "    - *Oversampling*. Increase the representation of minority classes by duplicating existing samples or generating synthetic ones.\n",
        "    - *Undersampling*. Reduce the number of majority class samples to balance the dataset.\n",
        "    - *Combined Resampling*. Employ both oversampling and undersampling to achieve a balanced dataset.\n",
        "  - **Algorithm-Level Techniques**:\n",
        "    - *Model Adjustment*. Modify algorithms to be more resilient to class imbalance without altering the training data distribution.\n",
        "    - *Ensemble Methods*. Utilize techniques like Random Forest or AdaBoost, which can handle imbalanced datasets effectively.\n",
        "  - **Cost-Sensitive Learning**:\n",
        "    *Assigning Costs*. Allocate higher misclassification costs to minority classes to ensure the model pays adequate attention to them.\n",
        "  - **Anomaly Detection**: *Treating Minority Classes as Anomalies*. In cases of extreme imbalance, consider minority classes as anomalies and apply appropriate detection techniques.\n",
        "\n",
        "  Selecting the appropriate strategy depends on the specific problem, dataset characteristics, and desired outcomes.\n",
        "\n",
        "  `Source`: https://www.isi-web.org/sites/default/files/2024-02/Handling-Data-Imbalance-in-Machine-Learning.pdf\n",
        "\n",
        "3. The accuracy can be misleading as a performance measure because it may not reflect the model's true predictive capabilities. This phenomenon is known as **the accuracy paradox**.\n",
        "\n",
        "  *Example*. Consider a dataset with 90% 'ham' (non-spam) emails and 10% 'spam' emails. A model that naively classifies all emails as 'ham' would achieve 90% accuracy, as it correctly identifies all 'ham' emails. However, it fails to detect any 'spam' emails, rendering it ineffective for spam detection.\n",
        "\n",
        "  <u>Key Points of **accuracy paradox**</u>:\n",
        "  - **High Accuracy, Low Predictive Power**: A high accuracy rate does not necessarily indicate a model's effectiveness, especially when it overlooks minority classes.\n",
        "  - **Importance of Precision and Recall**: Metrics such as precision (the proportion of positive identifications that are actually correct) and recall (the proportion of actual positives that were correctly identified) provide a more nuanced evaluation of model performance in imbalanced datasets.\n",
        "\n",
        "  `Source`: https://digestize.medium.com/stat-digest-the-idea-behind-accuracy-paradox-e79daa9fd917\n",
        "\n",
        "4. CIFAR-10 is *balanced* dataset that is proven by the following pie chart:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd907b86",
      "metadata": {
        "id": "bd907b86"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count the number of samples for each class in the training dataset\n",
        "train_class_counts = Counter(full_train_dataset.targets)\n",
        "\n",
        "# Count the number of samples for each class in the test dataset\n",
        "test_class_counts = Counter(test_dataset.targets)\n",
        "\n",
        "# Class names in CIFAR-10\n",
        "class_names = full_train_dataset.classes\n",
        "\n",
        "# Plot the class distribution for the training dataset\n",
        "plt.bar(class_names, [train_class_counts[i] for i in range(len(class_names))])\n",
        "plt.xlabel('Class Names')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Number of Samples per Class in CIFAR-10 (Training Dataset)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "sizes = [train_class_counts[i] for i in range(len(class_names))]\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sizes, labels=class_names, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Class Distribution in CIFAR-10 (Training Dataset)')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Training dataset class distribution:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}: {train_class_counts[i]} samples\")\n",
        "\n",
        "print(\"\\nTest dataset class distribution:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{class_name}: {test_class_counts[i]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fff764",
      "metadata": {
        "id": "f4fff764"
      },
      "source": [
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18cb829e",
      "metadata": {
        "id": "18cb829e"
      },
      "source": [
        "## **5: Improve the accuracy** (BONUS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c289bcd",
      "metadata": {
        "id": "5c289bcd"
      },
      "source": [
        "### 5.1: Custom model\n",
        "\n",
        "After successfully debugging the model, you'll notice that the accuracy on the CIFAR-10 dataset is only around 50-60%. Your second task is to improve the model's performance.\n",
        "\n",
        "How?\n",
        "\n",
        "*   Add more convolutional layers to capture higher-level features.\n",
        "*   Use Batch Normalization\n",
        "*   Add Dropout Layers\n",
        "\n",
        "Data Augmentation:\n",
        "*   Apply transformations like random cropping, flipping, and rotation.\n",
        "\n",
        "Hint: You CAN implement already pre-existing CNN architectures (do your research). As long as it is a CNN everything is fine.\n",
        "\n",
        "By the end of this section you should return the accuracy of your model on the test dataset.\n",
        "\n",
        "NB: by better score we mean at least +10% with respect to the previous model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WHlNsdE_ecfy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHlNsdE_ecfy",
        "outputId": "1652e711-b4ae-41d3-ad03-9cfde9b02191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k4mtPArXea9g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4mtPArXea9g",
        "outputId": "1b2b8ac0-dab4-48a3-a2be-b639f469b7b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ob9-70d7Gd8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob9-70d7Gd8d",
        "outputId": "791367f8-1789-4ee8-9a47-0d2bf8ca2c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Define transformations for the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wSu_agA5LRnp",
      "metadata": {
        "id": "wSu_agA5LRnp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CustomCNN_DataAugmented(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN_DataAugmented, self).__init__()\n",
        "\n",
        "        # First Convolutional Block\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Fourth Convolutional Block (New Layer)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Dropout layer (Reduced Dropout Rate)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "        # Fully connected layers with increased feature map size\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)  # Increased to 512 units\n",
        "        self.fc2 = nn.Linear(512, 10)   # Output size for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First Convolutional Block\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Fourth Convolutional Block\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)  # Output layer\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4xO4yz5jk8Bx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xO4yz5jk8Bx",
        "outputId": "876371ff-1847-491f-a4ee-a1f7dce71ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "0.49139968 0.48215827 0.44653124\n",
            "0.24703233 0.24348505 0.26158768\n"
          ]
        }
      ],
      "source": [
        "# Source: https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data\n",
        "import torch\n",
        "import numpy\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "imgs = [item[0] for item in train_dataset] # item[0] and item[1] are image and its label\n",
        "imgs = torch.stack(imgs, dim=0).numpy()\n",
        "\n",
        "# calculate mean over each channel (r,g,b)\n",
        "mean_r = imgs[:,0,:,:].mean()\n",
        "mean_g = imgs[:,1,:,:].mean()\n",
        "mean_b = imgs[:,2,:,:].mean()\n",
        "print(mean_r,mean_g,mean_b)\n",
        "\n",
        "# calculate std over each channel (r,g,b)\n",
        "std_r = imgs[:,0,:,:].std()\n",
        "std_g = imgs[:,1,:,:].std()\n",
        "std_b = imgs[:,2,:,:].std()\n",
        "print(std_r,std_g,std_b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bJt4JdfZB8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0bJt4JdfZB8",
        "outputId": "95a4047f-424c-470c-d4e5-9b134ad46974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),            # Random crop with 4-pixel padding\n",
        "    transforms.RandomHorizontalFlip(),               # Horizontal flip (50% prob it happens)\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    # To ensure zero centered input data\n",
        "    transforms.Normalize((0.49139968 , 0.48215827 , 0.44653124),   # CIFAR-10 mean for each channel\n",
        "                         (0.24703233 , 0.243, 0.24348505))      # CIFAR-10 std deviation for each channel\n",
        "])\n",
        "\n",
        "# It's important to apply normalization on test to ensure consistency, ofc no augmentation instead\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968 , 0.48215827 , 0.44653124),   # CIFAR-10 mean for each channel\n",
        "                         (0.24703233 , 0.243, 0.24348505))      # CIFAR-10 std deviation for each channel\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4486bead",
      "metadata": {
        "id": "4486bead"
      },
      "source": [
        "--------------------------------------------\n",
        "\n",
        "**Fill in the code in `libs/models/custom_cnn.py**\n",
        "\n",
        "--------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30575ec4",
      "metadata": {
        "id": "30575ec4"
      },
      "outputs": [],
      "source": [
        "# /content/drive/MyDrive/Homework02/libs/models/custom_cnn.py\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Homework02/libs/models')\n",
        "\n",
        "from custom_cnn import CustomCNN\n",
        "\n",
        "\n",
        "net = CustomCNN().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecba4705",
      "metadata": {
        "id": "ecba4705"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ba8675",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ba8675",
        "outputId": "9adfea75-8fbd-4f56-db70-3df15932be9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.3770\n",
            "Epoch 2, Loss: 1.0499\n",
            "Epoch 3, Loss: 0.9310\n",
            "Epoch 4, Loss: 0.8560\n",
            "Epoch 5, Loss: 0.8070\n",
            "Epoch 6, Loss: 0.7548\n",
            "Epoch 7, Loss: 0.7170\n",
            "Epoch 8, Loss: 0.6804\n",
            "Epoch 9, Loss: 0.6448\n",
            "Epoch 10, Loss: 0.6177\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=5e-05)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350ceb57",
      "metadata": {
        "id": "350ceb57"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7550f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd7550f6",
        "outputId": "2810af96-c51d-4b15-e710-080ea2b96e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the test images: 77.08%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c58dfb",
      "metadata": {
        "id": "f0c58dfb"
      },
      "source": [
        "\n",
        "### 5.2: Pretrained network\n",
        "In this exercise, you will start from scratch to adapt a pre-trained AlexNet model for the CIFAR-10 dataset.\n",
        "\n",
        "Instructions\n",
        "\n",
        "- Use torchvision.models to load a pre-trained AlexNet. Be sure to specify that the model should be pre-trained on ImageNet.\n",
        "\n",
        "- The CIFAR-10 dataset has 10 classes, so you need to update the model’s final layer to output 10 classes instead of the default 1000.\n",
        "\n",
        "- Replace the final fully connected layer in AlexNet’s classifier to output 10 classes.\n",
        "\n",
        "- To perform fine-tuning, freeze all layers except the newly added fully connected layer.\n",
        "\n",
        "- Move your model to the appropriate device (cuda if available). Define a device and ensure the model is moved to that device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba5ee586",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba5ee586",
        "outputId": "cdb96b59-ee26-4415-f05f-596d38ba5745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Libraries I need to start the code without rerunning the whole notebook\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3C5JokL6apsu",
      "metadata": {
        "id": "3C5JokL6apsu"
      },
      "outputs": [],
      "source": [
        "# Define a device and ensure the model is moved to that device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s9lzfrcCpj8H",
      "metadata": {
        "id": "s9lzfrcCpj8H"
      },
      "source": [
        "### Debugging info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Hsusuxsp9i8",
      "metadata": {
        "id": "0Hsusuxsp9i8"
      },
      "outputs": [],
      "source": [
        "model = models.alexnet(pretrained=True) # pretrained=True ensure to use the weights pre-trained on the ImageNet dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3pleGejTp_E7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pleGejTp_E7",
        "outputId": "4ca9fc13-2b14-4adc-f35f-dc3b7056e85b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PSUA7gW_rBpe",
      "metadata": {
        "id": "PSUA7gW_rBpe"
      },
      "source": [
        "### Differences between my structure vs AlexNet\n",
        "- AlexNet: Has 5 convolutional layers, mine has 4\n",
        "- Initial large kernel (11x11) to focus on higher-level features early on\n",
        "- Stride of 4 in the first layer aggressively reduce spatial dimensions initally\n",
        "- I used Max Pooling after each convolutional layer, while AlexNet uses 2 in early layers and 1 in the end\n",
        "- In the final output I used AdaptiveAvgPool2d\n",
        "- I also added Batch Normalization after each convolutional layer\n",
        "- AlexNet use a higher dropout rate of 0.5 (mine is 0.3)\n",
        "\n",
        "I think my solution provides a really efficient structure for CIFAR-10 dataset, more suitable for this simple dataset even if training was not enough cause Loss was func was still going down. On the other end, AlexNext leverage on his complexity and super tuned weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b8400e",
      "metadata": {
        "id": "32b8400e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "# Modify the final layer of AlexNet for 10 classes instead of 1000\n",
        "# so model.classifier[6] is the final fully connected layer\n",
        "model.classifier[6] = nn.Linear(in_features=model.classifier[6].in_features, out_features=10) # we keep input features as they are so the layer structure remains compatible with the rest of the network but we change output to 10 classes (for our need)\n",
        "\n",
        "# Freeze all layers except the final fully connected layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False # allow to fix (so not update) layers's weights during backpropagation\n",
        "for param in model.classifier[6].parameters():\n",
        "    param.requires_grad = True # instead ofc we train the last one\n",
        "\n",
        "# This approach makes the train faster while reduce overfitting since we have trained on a bigger dataset than CIFAR-10\n",
        "\n",
        "model = model.to(device) # Move model to device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09bfc3c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09bfc3c4",
        "outputId": "2d7a6d11-7d10-4f52-b6f0-393bff34e7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7283\n",
            "Finished Training\n",
            "Accuracy on the test images: 80.49%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-05)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test images: {100 * correct / total:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}