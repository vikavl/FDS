{"cells":[{"cell_type":"markdown","id":"1171fb23","metadata":{"id":"1171fb23","tags":[]},"source":["# **#2 Homework: Classification**\n","\n","**Fundamentals of Data Science - Winter Semester 2024**\n","\n","##### Matteo Migliarini (TA), Matteo Rampolla (TA) and Prof. Indro Spinelli\n","<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it>, <spinelli@di.uniroma1.it>\n","\n","---\n","\n","*Note: your task is to fill in the missing code where you see `\"YOUR CODE HERE\"` and the text part `\"WRITE YOUR TEXT HERE\"` part corresponding to each subproblem and produce brief reports on the results whenever necessary. Note also that a part of this missing code is also distributed in the python files in the folder `libs/`*\n","\n","As part of the homework, provide the answer to questions in this notebook report-like manner.\n","\n","After you have implemented all the missing code in the required sections, you will be able to run all the code without any errors.\n","\n","We kindly ask you to double-check this since **all** the delivered homework will be executed.\n","\n","The completed exercise should be handed in as a single notebook file. Use Markdown to provide equations. Use the code sections to provide your scripts and the corresponding plots.\n","\n","-------------------------------------\n","\n","**Submit it** by sending an email to:\n","\n","<migliarini.1886186@studenti.uniroma1.it>, <rampolla.1762214@studenti.uniroma1.it> and <spinelli@di.uniroma1.it> **by 29th November, 23:59**.\n","\n","-------------------------------------\n","\n","**Outline and Scores for #2 Homework:**\n","\n","\n","* **Question 1: Logistic Regression** *(6 points)*\n","  * **1.1: Log-likelihood and Gradient Ascent rule** (1 points)\n","  * **1.2: Implementation of Logistic Regression with Gradient Ascent** (2 points)\n","  * **1.3: Report** (3 points)\n","* **Question 2: Polynomial Expansion** *(7 points)*\n","  * **2.1: Polynomial features for logistic regression** (1 points)\n","  * **2.2: Plot the computed non-linear boundary** (2 point)\n","  * **2.4: Penalization** (4 points)\n","* **Question 3: Multinomial Classification** *(9  points)*\n","  * **3.1: Softmax Regression Model** (1 point)\n","  * **3.2: Coding** (3 points)\n","  * **3.3: Pipeline** (2 point)\n","  * **3.4: Hyperparameters** (1 point)\n","  * **3.5: Report** (2 point)\n","* **Question 4: First approach to CNNs** *(8 points)*\n","  * **4.1: Split the CIFAR-10 dataset** (1 point)\n","  * **4.2: Identify and Correct Errors in the CNN Model** (3 points)\n","  * **4.3: Training procedure** (2 points)\n","  * **4.4: Evaluate** (1 point)\n","  * **4.5: Report** (1 point)\n","* **Question 5: Improve the accuracy** (BONUS) *(5 points)*\n","  * **5.1: Custom model** (3 points)\n","  * **5.2: Pretrained Network** (2 points)\n","\n","**TOTAL POINTS ARE 35, MAXIMUM GRADE IS 30**\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"id":"a8b40d9c","metadata":{"id":"a8b40d9c"},"outputs":[],"source":["if True:\n","    %pip install -qqq numpy scipy matplotlib pandas scikit-learn seaborn tqdm"]},{"cell_type":"code","execution_count":null,"id":"3960c7ef","metadata":{"id":"3960c7ef"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np # imports a fast numerical programming library\n","import matplotlib.pyplot as plt # sets up plotting under plt\n","import pandas as pd # lets us handle data as dataframes\n","import seaborn as sns\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","%load_ext autoreload\n","%autoreload 2\n","\n","# sets up pandas table display\n","pd.set_option('display.width', 500)\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.notebook_repr_html', True)"]},{"cell_type":"markdown","id":"35c56914","metadata":{"id":"35c56914"},"source":["**Notation:**\n","\n","- $x^i$ is the $i^{th}$ feature vector\n","- $y^i$ is the expected outcome for the $i^{th}$ training example\n","- $m$ is the number of training examples\n","- $n$ is the number of features\n","\n","**Let's start by setting up our Python environment and importing the required libraries:**"]},{"cell_type":"markdown","id":"306465a9","metadata":{"id":"306465a9","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## 1: **Logistic Regression**"]},{"cell_type":"markdown","id":"ba37beb8","metadata":{"id":"ba37beb8"},"source":["### **1.1: Log-likelihood and Gradient Ascent Rule**\n","\n","Write the likelihood $L(\\theta)$ and log-likelihood $l(\\theta)$ of the parameters $\\theta$.\n","\n","Recall the probabilistic interpretation of the hypothesis $h_\\theta(x)= P(y=1|x;\\theta)$ and that $h_\\theta(x)=\\frac{1}{1+\\exp(-\\theta^T x)}$.\n","\n","Also derive the gradient $\\frac{\\delta l(\\theta)}{\\delta \\theta_j}$ of $l(\\theta)$ and write the gradient update equation."]},{"cell_type":"markdown","id":"663c4718","metadata":{"id":"663c4718"},"source":["-------------------------------------------------------\n","\n","**WRITE YOUR EQUATIONS HERE**\n","\n","- **Likelihood**:\n","\\begin{align}\n","L(\\theta) &=\n","\\end{align}\n","\n","- **Log-Likelihood**:\n","\n","\\begin{align}\n","l(\\theta) &=\n","\\end{align}\n","\n","- **Gradient of log-likelihood** (slide 5 p. 20):\n","\\begin{align}\n","\\frac{\\delta l(\\theta)}{\\delta \\theta_j} &=\n","\\end{align}\n","\n","- **Gradient update equation**:\n","For  $j=0,...,n$:\n","\\begin{equation}\n","\\theta_j =\n","\\end{equation}\n","\n","-------------------------------------------------------"]},{"cell_type":"markdown","id":"69909e04","metadata":{"id":"69909e04"},"source":["### **1.2: Logistic regression with Gradient Ascent**\n","\n","Define the sigmoid function `sigmoid`, then define the `LogisticRegression` class with the relative methods necessary to make predictions on an input, compute the log-likelihood and update its parameters.\n","Then define a function that takes in input such $X$, $y$ and the predictions $\\hat{y} = g(\\theta^{T}x)$ and computes the gradient of the log-likelihood.\n","Finally implement a function that takes in input such class and performs the training loop with the specified hyperparameters.\n","\n","Translate the equations you wrote above in code to learn the logistic regression parameters, $x^{(i)}_1$ and $x^{(i)}_2$ represent the two features for the $i$-th data sample $x^{(i)}$ and $y^{(i)}$ is its ground truth label.\n","\n","*Hint: even though by definition log likelihood and gradient ascent are defined by summations, for numerical stability it is advised to use the mean operation.*\n","\n","--------------------------------------------\n","\n","**Fill in the code in `libs/models/logisic_regression.py`, `libs/math.py/sigmoid()` and `libs/optim.py`**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"e4829929","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"e4829929","executionInfo":{"status":"error","timestamp":1730904163534,"user_tz":-60,"elapsed":454,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"34d13c0c-ac10-4bd0-c69b-b983fe5a2100"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'libs'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d0d475a4e39b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'libs'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from libs.models import LogisticRegression\n","from libs.optim import fit"]},{"cell_type":"markdown","id":"c2ca378a","metadata":{"id":"c2ca378a"},"source":["**Check your grad_l implementation:**\n","\n","`LogisticRegression.log_likelihood` applied to some random vectors should provide a value for `output_test` close to the `target_value` (defined below).\n","In other words, `error_test` should be close to 0.\n","\n","**Do not write below this line just run it**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"4df146f9","metadata":{"id":"4df146f9"},"outputs":[],"source":["target_value = -1\n","np.random.seed(1)\n","output_test = LogisticRegression.likelihood(np.random.random(100), np.random.randint(0, 2, 100))\n","error_test = np.abs(output_test - target_value)\n","print(\"Error: \", error_test)\n","assert error_test < 0.2, \"The output is not correct\""]},{"cell_type":"markdown","id":"f876a9e2","metadata":{"id":"f876a9e2"},"source":["#### Preprocessing\n","<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=800/>\n","\n","Now you'll load a dataset of penguins data. The dataset contains three species of penguins (Adelie, Gentoo and Chinstrap). Your goal will be to classify a penguin species based on their bill's length and body mass. First we'll load the dataset:"]},{"cell_type":"code","execution_count":null,"id":"eae9d063","metadata":{"id":"eae9d063"},"outputs":[],"source":["data = pd.read_csv(\"assets/train.csv\")\n","data.head()"]},{"cell_type":"markdown","id":"07ee1a83","metadata":{"id":"07ee1a83"},"source":["We want to train a classifier capable of understanding the difference between Adelie and Gentoo solely based on their bill's length and body mass. Thus in order to preprocess the data we:\n","1. Drop all the items with null data.\n","2. Remove the third species (Chinstrap) from the dataset.\n","3. Select the features we're interested in (`bill_length`, `body_mass`).\n","4. Select the label data and encode it in the values 0 and 1.\n","\n","<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png\" width=\"500\"/>"]},{"cell_type":"code","execution_count":null,"id":"0bda351f","metadata":{"id":"0bda351f"},"outputs":[],"source":["data.dropna(inplace=True)\n","data = data[data[\"species\"] != \"Chinstrap\"]\n","X = data[[\"bill_length\", \"body_mass\"]]\n","y = data[\"species\"].map({\"Adelie\": 0, \"Gentoo\": 1}).values"]},{"cell_type":"markdown","id":"3b3cb402","metadata":{"id":"3b3cb402"},"source":["It is recommended to normalize data when using machine learning techniques, so now normalize $X$ to have $\\mu=0, \\sigma=1$."]},{"cell_type":"code","execution_count":null,"id":"b446b8f0","metadata":{"id":"b446b8f0"},"outputs":[],"source":["##############################################\n","###             YOUR CODE HERE           #####\n","##############################################\n","X = X"]},{"cell_type":"markdown","id":"057099fb","metadata":{"id":"057099fb"},"source":["We add a column of 1's to $X$ to take into account the intercept."]},{"cell_type":"code","execution_count":null,"id":"118726ed","metadata":{"id":"118726ed"},"outputs":[],"source":["X[\"bias\"] = 1\n","# Reordering columns to have the bias term first (convention)\n","X = X[[\"bias\", \"bill_length\", \"body_mass\"]]\n","X"]},{"cell_type":"markdown","id":"d4c6c591","metadata":{"id":"d4c6c591"},"source":["#### Training\n","Now you'll use the class defined above to train a logistic regression model on classifying a group of penguins."]},{"cell_type":"code","execution_count":null,"id":"3d70727f","metadata":{"id":"3d70727f"},"outputs":[],"source":["# Initialize the model\n","model = LogisticRegression(num_features=X.shape[1])\n","\n","# Run Gradient Ascent method\n","n_iter = 50\n","log_l_history, _ = fit(model, X, y, lr=0.5, num_steps=n_iter)"]},{"cell_type":"markdown","id":"589ba43d","metadata":{"id":"589ba43d"},"source":["Let's plot the log likelihood over different iterations:\n"]},{"cell_type":"code","execution_count":null,"id":"d53b2578","metadata":{"id":"d53b2578"},"outputs":[],"source":["plt.plot(range(len(log_l_history)), log_l_history, \"b\")\n","plt.ylabel(\"log-likelihood(Theta)\")\n","plt.xlabel(\"Iterations\")"]},{"cell_type":"markdown","id":"f333bcd8","metadata":{"id":"f333bcd8"},"source":["Plot the data and the decision boundary:"]},{"cell_type":"code","execution_count":null,"id":"24c2e256","metadata":{"id":"24c2e256"},"outputs":[],"source":["plt.figure(figsize=(6,6))\n","sns.scatterplot(data=X, x=\"bill_length\", y=\"body_mass\", hue=data[\"species\"])\n","\n","x_range = np.linspace(X['bill_length'].min(), X['bill_length'].max(), 100)\n","theta_final = model.parameters\n","y_range = -(theta_final[0] + theta_final[1] * x_range) / theta_final[2]\n","plt.plot(x_range, y_range, c=\"red\")\n","\n","plt.xlim(X['bill_length'].min() - 0.2, X['bill_length'].max() + 0.2)\n","plt.ylim(X['body_mass'].min() - 0.2, X['body_mass'].max() + 0.2)"]},{"cell_type":"code","execution_count":null,"id":"df68aa89","metadata":{"id":"df68aa89"},"outputs":[],"source":["accuracy = ((model.predict(X) > 0.5) == y).mean()\n","print(f\"Accuracy: {accuracy}\")\n","assert accuracy > 0.6, \"The accuracy is too low\""]},{"cell_type":"markdown","id":"dfad244e","metadata":{"id":"dfad244e"},"source":["### **1.3: Report**"]},{"cell_type":"markdown","id":"9422a9aa","metadata":{"id":"9422a9aa"},"source":["1. Are we looking for a local minimum or a local maximum using the gradient ascent rule?\n","2. You have implemented the gradient ascent rule. Could we have also used gradient descent instead for the proposed problem? Why/Why not?\n","3. Let's deeply analyze how the learning rate $\\alpha$ and the number of iterations affect the final results. Run the algorithm you have written for different values of $\\alpha$ and the number of iterations and look at the outputs you get. Is the decision boundary influenced by these parameters change? Why do you think these parameters are affecting/not affecting the results?\n","4. What happens if you do not normalize the data? Try to run the algorithm without normalizing the data and see what happens. Why do you think this happens?\n"]},{"cell_type":"markdown","id":"e87caf56","metadata":{"id":"e87caf56"},"source":["-------------------------------------------------------\n","\n","\n","**WRITE YOUR ANSWER HERE:**\n","\n","1.\n","2.\n","3.\n","4.\n","5. *(feel free to add here screenshots or new code cells if needed)*\n","\n","-------------------------------------------------------"]},{"cell_type":"markdown","id":"e71ad46b","metadata":{"id":"e71ad46b","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## 2: **Polynomial Expansion**"]},{"cell_type":"markdown","id":"022266f7","metadata":{"id":"022266f7","tags":[]},"source":["### **2.1: Polynomial features for logistic regression**\n","\n","Define new features e.g., of 2nd and 3rd degree, and learn a logistic regression classifier by using the new features and the gradient ascent optimization algorithm defined in Question 1.\n","\n","In particular, consider a polynomial boundary with equation:\n","\n","\\begin{equation}\n","f(x_1, x_2) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1^2 + c_4 x_2^2 + c_5 x_1 x_2 + c_6 x_1^3 + c_7 x_2^3 + c_8 x_1^2 x_2 + c_9 x_1 x_2^2\n","\\end{equation}\n","\n","Therefore compute 7 new features: 3 new ones for the quadratic terms and 4 new ones for the cubic terms.\n","\n","Create new arrays by stacking $x$ and the new 7 features (in the order $x_1x_1, x_2x_2, x_1x_2, x_1x_1x_1, x_2x_2x_2, x_1x_1x_2, x_1x_2x_2$).\n","In particular create `x_new_quad` by additionally stacking $x$ with the quadratic features, and `x_new_cubic` by additionally stacking $x$ with the quadratic and the cubic features.\n","\n","**Do not write below this line just run it**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"e200aed3","metadata":{"id":"e200aed3"},"outputs":[],"source":["from sklearn.datasets import make_classification\n","X, y = make_classification(\n","    n_samples=700,\n","    n_features=2,\n","    n_informative=2,\n","    n_redundant=0,\n","    n_classes=2,\n","    n_clusters_per_class=1,\n","    class_sep=.3,\n","    random_state=89,\n",")\n","X = np.hstack([np.ones_like(X[:, [0]]), X])\n","X, X_val, y, y_val = train_test_split(X, y, test_size=200, random_state=42)\n","\n","sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y)"]},{"cell_type":"code","execution_count":null,"id":"18dc0176","metadata":{"id":"18dc0176"},"outputs":[],"source":["def get_polynomial(X, degree):\n","    \"\"\"\n","    Given an initial set of features, this function computes the polynomial features up to the given degree.\n","\n","    Args:\n","        X: the initial features matrix of shape (n_samples, 3) where the first column is the bias term\n","        degree: the degree of the polynomial\n","\n","    Returns:YOUR\n","        X: the final polynomial features\n","    \"\"\"\n","    if degree < 2:\n","        return X\n","\n","    features = np.ones(X.shape[0])\n","\n","    #####################################################\n","    ##                 YOUR CODE HERE                  ##\n","    #####################################################\n","    return features"]},{"cell_type":"markdown","id":"6aca7ebf","metadata":{"id":"6aca7ebf"},"source":["\n","**Do not write below this line just run it**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"611a2aee","metadata":{"id":"611a2aee"},"outputs":[],"source":["x_new_quad = get_polynomial(X, degree=2)\n","x_new_cubic = get_polynomial(X, degree=3)\n","print(x_new_quad.shape, x_new_cubic.shape)"]},{"cell_type":"markdown","id":"57a43fe0","metadata":{"id":"57a43fe0"},"source":["Now use the gradient ascent optimization algorithm to learn the models by maximizing the log-likelihood, both for the case of `x_new_quad` and `x_new_cubic`.\n","\n","\n","**Do not write below this line just run it**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"4ea40537","metadata":{"id":"4ea40537"},"outputs":[],"source":["n_iter = 50\n","model_lin = LogisticRegression(num_features=X.shape[1])\n","log_l_history,_ = fit(model_lin, X, y, lr=0.5, num_steps=n_iter)\n","\n","# Initialize model, in case of quadratic features\n","model_quad = LogisticRegression(num_features=x_new_quad.shape[1])\n","log_l_history_quad,_ = fit(model_quad, x_new_quad, y, lr=0.5, num_steps=n_iter)\n","\n","# Initialize model, in case of quadratic and cubic features\n","model_cubic = LogisticRegression(num_features=x_new_cubic.shape[1])\n","log_l_history_cubic,_ = fit(model_cubic, x_new_cubic, y, lr=0.5, num_steps=n_iter)\n","\n","log_l = np.stack([log_l_history, log_l_history_quad, log_l_history_cubic])\n","\n","log_l_df = pd.DataFrame(log_l.T, columns=[\"Linear\", \"Quadratic\", \"Cubic\"])\n","sns.lineplot(data=log_l_df, markers=True).set(\n","    xlabel=\"Iterations\", ylabel=\"Log Likelihood\", title=\"Log Likelihood History for Different Models\"\n",")"]},{"cell_type":"markdown","id":"b60667ee","metadata":{"id":"b60667ee"},"source":["### **2.2: Plot the computed non-linear boundary**\n","\n","First, define a boundary_function to compute the boundary equation for the input feature vectors $x_1$ and $x_2$, according to estimated parameters theta, both in the case of quadratic (theta_final_quad) and of quadratic and cubic features (theta_final_cubic). Refer for the equation to the introductory part of Question 2."]},{"cell_type":"code","execution_count":null,"id":"dcebc714","metadata":{"id":"dcebc714"},"outputs":[],"source":["def boundary_function(x1_vec, x2_vec, theta_final, degree):\n","    \"\"\"\n","    This function computes the boundary function for the given theta_final and degree.\n","\n","    Args:\n","        x1_vec: the x1 vector\n","        x2_vec: the x2 vector\n","        theta_final: the final theta\n","        degree: the degree of the polynomial\n","\n","    Returns:\n","        x1_vec: the x1 vector\n","        x2_vec: the x2 vector\n","        f: the boundary function\n","    \"\"\"\n","\n","    x1_vec, x2_vec = np.meshgrid(x1_vec, x2_vec)\n","\n","    #####################################################\n","    ##                 YOUR CODE HERE                  ##\n","    #####################################################\n","\n","    return x1_vec, x2_vec, f"]},{"cell_type":"markdown","id":"0b556291","metadata":{"id":"0b556291"},"source":["Now plot the decision boundaries corresponding to the theta_final_quad and theta_final_cubic solutions.\n","\n","**Do not write below this line just run it**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"6dd0d477","metadata":{"id":"6dd0d477"},"outputs":[],"source":["def plot_boundary_function(\n","    X: np.ndarray, y: np.ndarray, theta: np.ndarray, degree: int, n_points: int = 200\n",") -> None:\n","    \"\"\"\n","    This function plots the boundary function for the given theta and degree.\n","\n","    Args:\n","        X: the input data\n","        y: the input labels\n","        theta: the final theta\n","        degree: the degree of the polynomial\n","        n_points: the number of points to plot\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    x1_vec = np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, n_points)\n","    x2_vec = np.linspace(X[:, 2].min() - 1, X[:, 2].max() + 1, n_points)\n","\n","    x1_vec, x2_vec, f = boundary_function(x1_vec, x2_vec, theta, degree=degree)\n","    mesh_shape = int(np.sqrt(f.shape[0]))\n","\n","    sns.scatterplot(x=X[:, 1], y=X[:, 2], hue=y, legend=False)\n","    plt.contour(\n","        x1_vec, x2_vec, f.reshape((mesh_shape, mesh_shape)), colors=\"red\", levels=[0]\n","    )\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,3,1)\n","plot_boundary_function(X, y, model_lin.parameters, degree=1)\n","plt.title(\"Decision Boundary for Quadratic Features\")\n","plt.subplot(1,3,2)\n","plot_boundary_function(X, y, model_quad.parameters, degree=2)\n","plt.title(\"Decision Boundary for Quadratic Features\")\n","plt.subplot(1,3,3)\n","plot_boundary_function(X, y, model_cubic.parameters, degree=3)\n","plt.title(\"Decision Boundary for Cubic Features\");"]},{"cell_type":"markdown","id":"12908fe6","metadata":{"id":"12908fe6"},"source":["**Polynomial degree and overfitting**\n","\n","As the polynomial degree increases, the decision boundary becomes more and more complex. This can lead to overfitting, i.e. the model learns the training data too well, and it is not able to generalize to new data. This is a common problem in machine learning, and it is important to be able to detect it.\n","\n","In order to detect overfitting, we can split the dataset into a training set and a test set. The training set is used to learn the model, while the test set is used to evaluate the model performance on new data. If the model performs well on the training set, but it performs poorly on the test set, then we have overfitting.\n","\n","In this exercise, you are asked to plot the training and test accuracy as a function of the polynomial degree. Consider all the polynomial degrees from 1 to 20. For each polynomial degree, learn the model on the training set, and evaluate the accuracy on both the training and the test set. Additionally, visualize the decision boundary for the polynomials that give the **best** and the **worst** test accuracy for $\\texttt{degree} \\geq 2$."]},{"cell_type":"code","execution_count":null,"id":"0a8c83cf","metadata":{"id":"0a8c83cf"},"outputs":[],"source":["from sklearn.preprocessing import normalize\n","\n","def fit_polynomials(X, y, X_test, y_test, degrees, lr, num_steps, architecture = LogisticRegression):\n","    \"\"\"\n","    This function fits a logistic regression model for each degree in the degrees list.\n","    \"\"\"\n","    X = normalize(X)\n","    X_test = normalize(X_test)\n","\n","    thetas = []\n","    accuracy_scores_train, accuracy_scores_test = [], []\n","    for degree in tqdm(degrees):\n","        x_new = get_polynomial(X, degree=degree)\n","\n","        model = architecture(num_features=x_new.shape[1])\n","        fit(model, x_new, y, lr=lr, num_steps=num_steps)\n","\n","        thetas.append(model.parameters)\n","        y_hat_train = model.predict(x_new) > 0.5\n","        accuracy_scores_train.append(accuracy_score(y, y_hat_train))\n","        y_hat_test = model.predict(get_polynomial(X_test, degree=degree)) > 0.5\n","        accuracy_scores_test.append(accuracy_score(y_test, y_hat_test))\n","\n","    return thetas, accuracy_scores_train, accuracy_scores_test"]},{"cell_type":"code","execution_count":null,"id":"c28971c0","metadata":{"id":"c28971c0"},"outputs":[],"source":["degrees = np.arange(1, 20)\n","np.random.seed(42)\n","thetas, accuracy_scores_train, accuracy_scores_test = fit_polynomials(\n","    X, y, X_val, y_val, degrees=degrees, lr=0.5, num_steps=500, architecture=LogisticRegression\n",")\n","sns.lineplot(x=degrees, y=accuracy_scores_train, label=\"Train\")\n","sns.lineplot(x=degrees, y=accuracy_scores_test,  label=\"Test\")\n","plt.xlabel(\"Degree\")\n","plt.ylabel(\"Accuracy Score\")\n","plt.xticks(degrees)\n","plt.show()"]},{"cell_type":"markdown","id":"c8c0052d","metadata":{"id":"c8c0052d"},"source":["Plot the best and the worst decision boundaries for $\\texttt{degree} \\geq 2$.\n","\n","--------------------------------------------\n","**Write your code below this line**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"7c483b33","metadata":{"id":"7c483b33"},"outputs":[],"source":["# Plot worst model"]},{"cell_type":"code","execution_count":null,"id":"2aa71ea4","metadata":{"id":"2aa71ea4"},"outputs":[],"source":["# Plot best model"]},{"cell_type":"markdown","id":"e6eae9e4","metadata":{"id":"e6eae9e4"},"source":["#### **Report**\n","Write now your considerations. Discuss in particular:\n","1. Look back at the plots you have generated. What can you say about the differences between the linear, quadratic, and cubic decision boundaries? Can you say if the model is improving in performances, increasing the degree of the polynomial? Do you think you can incur in underfitting increasing more and more the degree?\n","2. Look at the plot of the training and test accuracy as a function of the polynomial degree. What can you say about the differences between the training and test accuracy? What can you say about the differences between the best and the worst test accuracy? In general, is it desirable to have a very complex decision boundary, i.e. a very high degree of the polynomial? Discuss and motivate your answer.\n","3. In general what are some properties of the dataset that makes it more prone to overfitting? Discuss their impact.\n"]},{"cell_type":"markdown","id":"08abefa0","metadata":{"id":"08abefa0"},"source":["-------------------------------------------------------\n","\n","\n","**WRITE YOUR ANSWER HERE:**\n","\n","1.\n","2.\n","3.\n","\n","\n","\n","-------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"5a3e29e0","metadata":{"id":"5a3e29e0"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"1f456b6e","metadata":{"id":"1f456b6e"},"source":["### **2.4: Weight Penalization**\n","\n","Look at how complicated the decision boundaries become as you increase the degree. Can we improve this and prevent overfitting?\n","When dealing with overfitting one frequent solution is to use a weigth penalization technique like L2 or L1 penalization.\n","\n","In our case we'll use L2 regularization. In this way the regularized likelihood will be:\n","$$\n","\\texttt{Likelihood}_{reg}(\\theta) = \\texttt{Likelihood}(\\theta) - \\frac{\\lambda}{2n} \\sum^n_i \\theta_i^2\n","$$\n","Thus we can derive the update rule as:\n","\\begin{equation}\n","\\theta_j:= \\theta_j + \\alpha( \\frac{\\partial l(\\theta_j)}{\\partial \\theta_j} -  \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{\\lambda}{2} \\theta_j^2 \\right ) )\n","\\end{equation}\n","\n","Calculating the second term of the update rule it's just a matter of analytically solving a simple gradient, do it, and then implement it by extending the `LogisticRegression` class:\n","\n","--------------------------------------------\n","\n","**Fill in the code in `libs/models/logisic_regression_penalized.py`**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"fb7cc62c","metadata":{"id":"fb7cc62c"},"outputs":[],"source":["from libs.models import LogisticRegressionPenalized"]},{"cell_type":"code","execution_count":null,"id":"19371cf3","metadata":{"id":"19371cf3"},"outputs":[],"source":["X,y = make_classification(\n","    n_samples=500,\n","    n_features=100,\n","    n_informative=50,\n","    n_redundant=25,\n","    n_classes=2, random_state=42)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","np.random.seed(42)\n","\n","lr = LogisticRegression(X.shape[1])\n","likelihood_history, val_loss_history = fit(lr, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n","\n","penalized_lt = LogisticRegressionPenalized(X.shape[1], 2)\n","pen_history, pen_val_history = fit(penalized_lt, X_train, y_train, X_val, y_val, lr=1e-2, num_steps=200)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(-likelihood_history[2:], label=\"Train\", color=\"violet\")\n","plt.plot(val_loss_history[2:], label=\"Test\", color='teal')\n","plt.plot(-pen_history[2:], label=\"Train - penalized\", color=\"violet\", linestyle=\"--\")\n","plt.plot(pen_val_history[2:], label=\"Test - penalized\", color=\"teal\", linestyle=\"--\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Non-penalized\")\n","plt.legend()"]},{"cell_type":"markdown","id":"58bee8a3","metadata":{"id":"58bee8a3"},"source":["Now, evaluate the Penalized Logistic Regression for each value of $\\lambda \\in [0,3]$ and find the one that performs the best:"]},{"cell_type":"code","execution_count":null,"id":"cb84e3bf","metadata":{"id":"cb84e3bf"},"outputs":[],"source":["lambdas = np.arange(0, 3, 0.1)\n","losses = []\n","\n","for lambda_ in lambdas:\n","    ##############################################\n","    ###         COMPLETE THIS FOR-LOOP         ###\n","    ##############################################\n","    pass\n","\n","if len(losses) > 0:\n","    sns.lineplot(x=lambdas, y=losses, label=\"Validation Loss\").set(\n","        xlabel=\"Lambda\", ylabel=\"Loss\", title=\"Validation Loss vs Lambda\"\n","    )\n","    print(f\"Best lambda: {lambdas[np.argmin(losses)]}\")"]},{"cell_type":"markdown","id":"8a9a196b","metadata":{"id":"8a9a196b"},"source":["#### Report\n","Write now your considerations. In particular:\n","1. What happens when we use a non-penalized logistic regression?\n","2. Observe the plot of the Train and Validation losses in the penalized vs non penalized case. In which case is the Train loss better? Can you explain why?\n","3. What is the convergence rate? How is it influenced by the penalization?\n","\n","-------------------------------------------------------\n","\n","\n","**WRITE YOUR ANSWER HERE:**\n","\n","1.\n","2.\n","3.\n","\n","-------------------------------------------------------"]},{"cell_type":"markdown","id":"7faba031","metadata":{"id":"7faba031","tags":[]},"source":["## 3: **Multinomial Classification**"]},{"cell_type":"markdown","id":"e107612c","metadata":{"id":"e107612c"},"source":["### **3.1: Softmax Regression Model**\n","\n","In the multinomial classification we generally have $K>2$ classes. So the label for the $i$-th sample $X_i$ is $y_i\\in\\{1,...,K\\}$, where $i=1,...,N$. The output class for each sample is estimated by returning a score $s_i$ for each of the K classes. This results in a vector of scores of dimension K.\n","In this exercise we'll use the *Softmax Regression* model, which is the natural extension of *Logistic Regression* for the case of more than 2 classes. The score array is given by the linear model:\n","\n","\\begin{align*}\n","s_i =  X_i \\theta\n","\\end{align*}\n","\n","Scores may be interpreted probabilistically, upon application of the function *softmax*. The position in the vector with the highest probability will be predicted as the output class. The probability of the class k for the $i$-th data sample is:\n","\n","\\begin{align*}\n","p_{ik} = \\frac{\\exp(X_i \\theta_k)}{\\sum_{j=1}^K(X_i \\theta_j))}\n","\\end{align*}\n","\n","We will adopt the *Cross Entropy* loss and optimize the model via *Gradient Descent*.\n","In the first of this exercise we have to:\n","-    Write the equations of the Cross Entropy loss for the Softmax regression model;\n","-    Compute the equation for the gradient of the Cross Entropy loss for the model, in order to use it in the gradient descent algorithm.\n","\n","#### A bit of notation\n","\n","*  N: is the number of samples\n","*  K: is the number of classes\n","*  X: is the input dataset and it has shape (N, H) where H is the number of features\n","*  y: is the output array with the labels; it has shape (N, 1)\n","*  $\\theta$: is the parameter matrix of the model; it has shape (H, K)"]},{"cell_type":"markdown","id":"753c8304","metadata":{"id":"753c8304"},"source":["--------------------------------------------\n","**Write you equation below this line**\n","\n","--------------------------------------------"]},{"cell_type":"markdown","id":"10170a74","metadata":{"id":"10170a74"},"source":["\\begin{align*}\n","L(\\theta) = ...\n","\\end{align*}\n","\n","\\begin{align*}\n","Loss(\\theta) = ...\n","\\end{align*}\n","\n","\n","\\begin{align*}\n","\\nabla_{\\theta_k} L(\\theta) = ...\n","\\end{align*}\n"]},{"cell_type":"markdown","id":"60ccc0db","metadata":{"id":"60ccc0db"},"source":["### **3.2: Coding**\n","\n","We are using the CIFAR-10 dataset for this exercise. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It has 50,000 training images and 10,000 test images. The dataset was established by the Canadian Institute For Advanced Research (CIFAR), and it has become a standard benchmark for machine learning algorithms, especially in the area of image classification."]},{"cell_type":"code","execution_count":null,"id":"b9e56f3a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"b9e56f3a","executionInfo":{"status":"error","timestamp":1730903878796,"user_tz":-60,"elapsed":94185,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"47cac39e-f029-429b-815b-88563b905b6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to assets/cifar10/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [01:00<00:00, 2.80MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting assets/cifar10/cifar-10-python.tar.gz to assets/cifar10\n","Files already downloaded and verified\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'np' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fe3fe39f1fd1>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","cifar_dir = \"assets/cifar10\"\n","transform = transforms.Compose([transforms.ToTensor()])\n","train_data = datasets.CIFAR10(\n","    root=cifar_dir, train=True, download=True, transform=transform\n",")\n","test_data = datasets.CIFAR10(\n","    root=cifar_dir, train=False, download=True, transform=transform\n",")\n","\n","# Convert labels to one-hot encoded format\n","def one_hot_encode(y, num_classes=10):\n","    encoded = np.zeros((len(y), num_classes))\n","    for i, val in enumerate(y):\n","        encoded[i, val] = 1\n","    return encoded\n","\n","# Evaluate the accuracy of the predictions\n","def compute_accuracy(predictions, true_labels):\n","    correct_predictions = np.sum(predictions == true_labels)\n","    total_predictions = len(true_labels)\n","    return correct_predictions / total_predictions\n","\n","# Preprocess the data\n","X_train = [img.reshape(-1).numpy() for img, _ in train_data]\n","X_train = np.array(X_train)\n","y_train = [label for _, label in train_data]\n","\n","X_val = [img.reshape(-1).numpy() for img, _ in test_data]\n","X_val = np.array(X_val)\n","y_val = [label for _, label in test_data]\n","\n","\n","# Add bias term to X\n","X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","\n","y_train_onehot = one_hot_encode(y_train)\n","y_test_onehot = one_hot_encode(y_val)"]},{"cell_type":"markdown","id":"508f30d9","metadata":{"id":"508f30d9"},"source":["*Hint: consider the labels as one-hot vector. This will allow matrix operations (element-wise multiplication and summation).*"]},{"cell_type":"markdown","id":"5d63efe6","metadata":{"id":"5d63efe6"},"source":["Now implement a classifier for Multinomial Classification using the `softmax` function. Again, implement it as a class with the methods:\n","- `predict`\n","- `predict_labels`\n","- `likelihood` *(Here you need to implement the Cross Entropy Loss)*\n","- `update_theta`\n","- `compute_gradient` to compute the Jacobian $\\nabla$\n","\n","Note that this this you don't need to reimplement the `fit()` function since the training loop you defined above works also for a Multinomial Classifier, provided that this is structured with the previously mentioned methods.\n","\n","--------------------------------------------\n","\n","**Fill in the code in `libs/models/multinomial.py` and `libs/math.py/softmax()`**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"4bbca091","metadata":{"id":"4bbca091"},"outputs":[],"source":["from libs.models import SoftmaxClassifier\n","from libs.optim import fit"]},{"cell_type":"markdown","id":"8c21abf9","metadata":{"id":"8c21abf9"},"source":["--------------------------------------------\n","\n","**Do not write below this line just run it**\n","\n","--------------------------------------------\n","\n","*Execution can take around 10 minutes*"]},{"cell_type":"code","execution_count":null,"id":"7353b3df","metadata":{"id":"7353b3df"},"outputs":[],"source":["# Apply gradient descent to optimize theta\n","alpha = 0.01\n","iterations = 500\n","H, K = X_train.shape[1], 10  # number of features and number of classes\n","model = SoftmaxClassifier(num_features=H, num_classes=K)\n","loss_history, _ = fit(model, X_train, y_train_onehot, lr=alpha, num_steps=iterations)\n","\n","# Make predictions on the training and test data\n","train_predictions = model.predict_labels(X_train)\n","test_predictions = model.predict_labels(X_val)\n","\n","train_accuracy = compute_accuracy(train_predictions, y_train)\n","test_accuracy = compute_accuracy(test_predictions, y_val)\n","\n","print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n","print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"]},{"cell_type":"markdown","id":"46307d23","metadata":{"id":"46307d23"},"source":["### **3.3: Pipeline**\n","Now you're going to use `scikit-learn` library to build a pipeline of operations to redo everything we've done so far in the homework. First we have loaded the required modules and the penguins dataset.\n","\n","---\n","\n","Then here you'll build the pipeline. We need four items:\n","1. The Numerical Transformer, to handle the preprocessing of numerical columns, by:\n","    - Imputing missing values with their mean\n","    - Enrich the features with a 3-rd degree polynomial expansion\n","    - Scaling of the features to $\\mu=0, \\sigma=1$\n","2. The Categorical Transformer, to handle the preprocessing of categorical values, by:\n","    - Imputing the missing values with the most frequent value\n","    - Encode the features in a one-hot vector.\n","3. The Preprocessor: a ColumnTransformer that distributed the numerical columns to the numerical transformer and the categorical columns to the categorical tranformer.\n","4. The final Pipeline, which contains the preprocessor and the classfier of your choice (in this case `KNeighborsClassifier`)"]},{"cell_type":"code","execution_count":null,"id":"ffae09bf","metadata":{"id":"ffae09bf"},"outputs":[],"source":["data = pd.read_csv('assets/train.csv')\n","\n","X_train, X_val, y_train, y_val = train_test_split(data.drop('species', axis=1), data.species, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"91db325f","metadata":{"id":"91db325f"},"outputs":[],"source":["numerical_cols = ['bill_length', 'bill_depth', 'flipper_length', 'body_mass']\n","categorical_cols = ['island', 'sex']\n","\n","##############################################\n","###          FILL IN THIS CODE           #####\n","##############################################\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import make_pipeline\n","## Import everything you need here\n","\n","numeric_transformer = make_pipeline(\n",")\n","\n","categoric_transfomer = make_pipeline(\n",")\n","\n","preprocessor = ColumnTransformer(transformers=[\n","    ('num', _, _),\n","    ('cat', _, _)\n","], remainder='drop')\n","\n","pipe = make_pipeline()\n","if len(pipe.named_steps)>0:\n","    display(pipe)"]},{"cell_type":"markdown","id":"9a20cc4b","metadata":{"id":"9a20cc4b"},"source":["Now, we can use this pipeline to preprocess the input data and fit a classifier. Leveraging `scikit-learn`'s pipelines allows you to:\n","- Define the entire chain of operations in a structured way, which is especially useful for cleaning and transforming data.\n","- Separate the definition of operations from their execution, creating a clean and organized workflow.\n","\n","This approach makes it easier to manage complex preprocessing steps while maintaining readability and clarity in your code."]},{"cell_type":"code","execution_count":null,"id":"1829845c","metadata":{"id":"1829845c"},"outputs":[],"source":["pipe.fit(X_train, y_train)\n","\n","y_pred = pipe.predict(X_val)\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"markdown","id":"37fd86ac","metadata":{"id":"37fd86ac"},"source":["This is nice but can we improve it? In defining the pipeline you certainly used some fixed hyperparameters, for example the number of neighbors or the degree of the polynomial expansion.\n","\n","First, let's look at the list of hyperparameters:"]},{"cell_type":"code","execution_count":null,"id":"4dcfd619","metadata":{"id":"4dcfd619"},"outputs":[],"source":["hparams = pipe.get_params()\n","for hp, val in hparams.items():\n","    if type(val) not in [int, float, str]:\n","        continue\n","    print(f\"{hp}: {val}\")"]},{"cell_type":"markdown","id":"7a99eea0","metadata":{"id":"7a99eea0"},"source":["Some of these hyperparameters are set to their default values, while others are explicitly defined. However, any data scientist knows that hyperparameters should not be arbitrarily chosen; instead, they should be optimized through **Cross-Validation**.\n","\n","We can leverage the compositionality of `scikit-learn` by incorporating the pipeline into a `GridSearchCV` class. This allows you to easily define a grid of parameters to val and automatically perform cross-validation over the combinations.\n","\n","Choose at least 2 values for at least 3 hyperparameters. val their impact on the model and find the best combination."]},{"cell_type":"code","execution_count":null,"id":"3389cc26","metadata":{"id":"3389cc26"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","grid = dict(\n",")\n","\n","pipe_cv = None\n","if pipe_cv is not None:\n","    pipe_cv.fit(X_train, y_train)\n","\n","    print(f\"Best score: {pipe_cv.best_score_}\")\n","    for hp, val in pipe_cv.best_params_.items():\n","        print(f\"{hp}: {val}\")"]},{"cell_type":"markdown","id":"054a24ba","metadata":{"id":"054a24ba"},"source":["`GridSearchCV` doesn't only find the best combination of hyperparmeters, but it also refits the model with the best hyperparameters it finds. Let's val this new model:"]},{"cell_type":"code","execution_count":null,"id":"d76df863","metadata":{"id":"d76df863"},"outputs":[],"source":["y_pred = pipe_cv.predict(X_val)\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"markdown","id":"661c4ac8","metadata":{"id":"661c4ac8"},"source":["#### Report\n","1. How many combinations has your gridsearch tried?\n","2. Make a plot with the results of your hyperparameter grid\n","3. Do you notice any trend in the performance of certain hyperparameters?\n","4. Do the classifiers obtain the same accuracy on train and val sets? If not, try to give an explanation.\n","5. With the choice of hyperparameters you made, do you notice any trade-off between accuracy and compute power? Show with a plot."]},{"cell_type":"code","execution_count":null,"id":"be3ce062","metadata":{"id":"be3ce062"},"outputs":[],"source":["cv_res = pd.DataFrame(pipe_cv.cv_results_)\n","cv_res.columns = [col.split('__')[-1] for col in cv_res.columns]\n","\n","##############################################\n","###                YOUR CODE HERE         ####\n","##############################################"]},{"cell_type":"code","execution_count":null,"id":"5239302b","metadata":{"id":"5239302b"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"db6e95b8","metadata":{"id":"db6e95b8"},"source":["--------------------------------------------\n","\n","**Write your answer below this line**\n","\n","1.\n","2.\n","3.\n","4.\n","5.\n","\n","--------------------------------------------"]},{"cell_type":"markdown","id":"8a0a3c8e","metadata":{"id":"8a0a3c8e"},"source":["## **4: Debugging a CNN with Shape Errors**\n","\n","You are provided with a CNN model intended to classify images from the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. However, the model contains shape mismatches between layers due to intentional errors. Your first task is to identify and fix these errors to make the model functional.*testo in corsivo*"]},{"cell_type":"code","execution_count":null,"id":"d941ac13","metadata":{"id":"d941ac13","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730903888242,"user_tz":-60,"elapsed":922,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"95cff84b-82ec-4928-a899-c2b85ce99c79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, random_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","\n","# Check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","id":"776d671f","metadata":{"id":"776d671f"},"source":["### 4.1: Split the CIFAR-10 Dataset"]},{"cell_type":"markdown","id":"0881a353","metadata":{"id":"0881a353"},"source":["Instructions:\n","\n","* Define the Split Sizes:\n","Calculate the sizes for the training and validation datasets. Allocate ***80% of the training*** data for the training set and *20% for the validation set*.\n","\n","* Split the Dataset:\n","Use `torch.utils.data.random_split` to create the training and validation datasets from the original training dataset.\n","\n","* Create Data Loaders:\n","Create data loaders for the training, validation, and test datasets using torch.utils.data.DataLoader with a ***batch size of 64***. Ensure that the training data is ***shuffled***.\n","\n","* Print the size of each dataset (train, test, val)"]},{"cell_type":"code","execution_count":null,"id":"c5b82813","metadata":{"id":"c5b82813"},"outputs":[],"source":["# Define transformations for the data that we will use\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","# Load the CIFAR-10 dataset\n","full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","#####################################################\n","##              YOUR CODE HERE                     ##\n","#####################################################\n","\n","# use these names for the data loaders\n","train_loader = _\n","test_loader = _\n","val_loader = _"]},{"cell_type":"markdown","id":"96ba0401","metadata":{"id":"96ba0401"},"source":["### 4.2: Identify and Correct Errors in the CNN Model\n"]},{"cell_type":"markdown","id":"5806744d","metadata":{"id":"5806744d"},"source":["In this exercise, you will analyze an intentionally incorrect implementation of a Convolutional Neural Network model. Your task is to identify the errors in the `PoorPerformingCNN` class and correct them to ensure the model works properly for the CIFAR-10 dataset.\n","\n","--------------------------------------------\n","\n","**Fill in the code in `libs/models/poor_cnn.py**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"84d190e2","metadata":{"id":"84d190e2"},"outputs":[],"source":["from libs.models import PoorPerformingCNN\n","\n","net = PoorPerformingCNN()"]},{"cell_type":"markdown","id":"5bfb7e81","metadata":{"id":"5bfb7e81"},"source":["Loss Function and optimizer"]},{"cell_type":"code","execution_count":null,"id":"f73233d5","metadata":{"id":"f73233d5"},"outputs":[],"source":["# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)"]},{"cell_type":"markdown","id":"05b442d3","metadata":{"id":"05b442d3"},"source":["### 4.3: Training procedure\n","\n","In this exercise, you will complete the training and validation loop of a neural network model. Your task is to compute and store the average training loss, average validation loss, and the corresponding accuracies for each epoch.\n","\n","Instructions:\n","\n","* **Training Phase**:\n","After computing the average training loss (avg_train_loss), you need to calculate the training accuracy based on the model's predictions and append the calculated training accuracy to the train_accuracies list.\n","\n","* **Validation Phase**:\n","After calculating the average validation loss (avg_val_loss), you need to calculate the validation accuracy based on the validation dataset and append the calculated validation accuracy to the val_accuracies list. (the same as befor but for the val)"]},{"cell_type":"code","execution_count":null,"id":"3858bb39","metadata":{"id":"3858bb39"},"outputs":[],"source":["# Initialize lists to store metrics\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","\n","num_epochs = 10\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    total_batches = 0\n","\n","    # Training Phase\n","    net.train()\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = net(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        total_batches += 1\n","\n","    #####################################################\n","    ##              YOUR CODE HERE                     ##\n","    #####################################################\n","\n","    # Calculate average loss for the training epoch\n","    avg_train_loss = _\n","    train_losses.append(avg_train_loss)\n","\n","    # Calculate training accuracy\n","    y_pred = _\n","    train_accuracy = _\n","    train_accuracies.append(train_accuracy)\n","\n","    #####################################################\n","    ##              END OF YOUR CODE                   ##\n","    #####################################################\n","\n","    print(f'Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n","\n","    # Validation Phase\n","    net.eval()\n","    val_running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for val_data in val_loader:\n","            val_inputs, val_labels = val_data\n","\n","            # Forward pass\n","            val_outputs = net(val_inputs)\n","\n","            # Compute loss\n","            val_loss = criterion(val_outputs, val_labels)\n","            val_running_loss += val_loss.item()\n","\n","            # Calculate accuracy\n","            _, val_predicted = torch.max(val_outputs.data, 1)\n","            total += val_labels.size(0)\n","            correct += (val_predicted == val_labels).sum().item()\n","\n","\n","    #####################################################\n","    ##              YOUR CODE HERE                     ##\n","    #####################################################\n","\n","    # Calculate average loss for the validation epoch\n","    avg_val_loss = _\n","    val_losses.append(avg_val_loss)\n","\n","    # Calculate validation accuracy\n","    val_accuracy = _\n","    val_accuracies.append(val_accuracy)\n","\n","    #####################################################\n","    ##              END OF YOUR CODE                   ##\n","    #####################################################\n","\n","    print(f'Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","print('Finished Training')\n"]},{"cell_type":"markdown","id":"6bb35d88","metadata":{"id":"6bb35d88"},"source":["### 4.4: Evaluate"]},{"cell_type":"code","execution_count":null,"id":"e248e772","metadata":{"id":"e248e772"},"outputs":[],"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        #####################################################\n","        ##              YOUR CODE HERE                     ##\n","        #####################################################\n","\n","        total += _\n","        correct += _\n","\n","        #####################################################\n","        ##              END OF YOUR CODE                   ##\n","        #####################################################\n","\n","print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"]},{"cell_type":"markdown","id":"2df9755d","metadata":{"id":"2df9755d"},"source":["### 4.5: Report\n","\n","1. What challenges can class imbalance introduce when training a machine learning model?\n","\n","2. What are some strategies to address class imbalance in your dataset or training process?\n","\n","3. Why might accuracy alone be misleading as a performance measure in the presence of class imbalance\n","\n","4. Is the cifar-10 and imbalanced dataset? plot the number of samples for each classes inside the cifar 10 dataset.\n","\n","-------------------------------------------------------\n","\n","\n","**WRITE YOUR ANSWER HERE:**\n","\n","1.\n","2.\n","3.\n","4.\n","\n","-------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"bd907b86","metadata":{"id":"bd907b86"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"18cb829e","metadata":{"id":"18cb829e"},"source":["## **5: Improve the accuracy** (BONUS)"]},{"cell_type":"markdown","id":"5c289bcd","metadata":{"id":"5c289bcd"},"source":["### 5.1: Custom model\n","\n","After successfully debugging the model, you'll notice that the accuracy on the CIFAR-10 dataset is only around 50-60%. Your second task is to improve the model's performance.\n","\n","How?\n","\n","*   Add more convolutional layers to capture higher-level features.\n","*   Use Batch Normalization\n","*   Add Dropout Layers\n","\n","Data Augmentation:\n","*   Apply transformations like random cropping, flipping, and rotation.\n","\n","Hint: You CAN implement already pre-existing CNN architectures (do your research). As long as it is a CNN everything is fine.\n","\n","By the end of this section you should return the accuracy of your model on the test dataset.\n","\n","NB: by better score we mean at least +10% with respect to the previous model.\n","\n","\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHlNsdE_ecfy","executionInfo":{"status":"ok","timestamp":1731366116190,"user_tz":-60,"elapsed":3901,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"1652e711-b4ae-41d3-ad03-9cfde9b02191"},"id":"WHlNsdE_ecfy","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Check if GPU is available\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4mtPArXea9g","executionInfo":{"status":"ok","timestamp":1730922253413,"user_tz":-60,"elapsed":3223,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"1b2b8ac0-dab4-48a3-a2be-b639f469b7b6"},"id":"k4mtPArXea9g","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# Define transformations for the data\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Load the CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Create data loaders\n","batch_size = 64\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ob9-70d7Gd8d","executionInfo":{"status":"ok","timestamp":1731366155890,"user_tz":-60,"elapsed":1803,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"791367f8-1789-4ee8-9a47-0d2bf8ca2c5b"},"id":"ob9-70d7Gd8d","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","class CustomCNN_DataAugmented(nn.Module):\n","    def __init__(self):\n","        super(CustomCNN_DataAugmented, self).__init__()\n","\n","        # First Convolutional Block\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","\n","        # Second Convolutional Block\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","\n","        # Third Convolutional Block\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","\n","        # Fourth Convolutional Block (New Layer)\n","        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(256)\n","\n","        # Dropout layer (Reduced Dropout Rate)\n","        self.dropout = nn.Dropout(0.4)\n","\n","        # Fully connected layers with increased feature map size\n","        self.fc1 = nn.Linear(256 * 4 * 4, 512)  # Increased to 512 units\n","        self.fc2 = nn.Linear(512, 10)   # Output size for 10 classes\n","\n","    def forward(self, x):\n","        # First Convolutional Block\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.max_pool2d(x, 2)\n","\n","        # Second Convolutional Block\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.max_pool2d(x, 2)\n","\n","        # Third Convolutional Block\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.max_pool2d(x, 2)\n","\n","        # Fourth Convolutional Block\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = F.max_pool2d(x, 2)\n","\n","        x = self.dropout(x)\n","        x = x.view(-1, 256 * 4 * 4)\n","\n","        # Fully Connected Layers\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)  # Output layer\n","\n","        return x\n"],"metadata":{"id":"wSu_agA5LRnp"},"id":"wSu_agA5LRnp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Source: https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data\n","import torch\n","import numpy\n","import torchvision.datasets as datasets\n","from torchvision import transforms\n","\n","cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n","\n","imgs = [item[0] for item in train_dataset] # item[0] and item[1] are image and its label\n","imgs = torch.stack(imgs, dim=0).numpy()\n","\n","# calculate mean over each channel (r,g,b)\n","mean_r = imgs[:,0,:,:].mean()\n","mean_g = imgs[:,1,:,:].mean()\n","mean_b = imgs[:,2,:,:].mean()\n","print(mean_r,mean_g,mean_b)\n","\n","# calculate std over each channel (r,g,b)\n","std_r = imgs[:,0,:,:].std()\n","std_g = imgs[:,1,:,:].std()\n","std_b = imgs[:,2,:,:].std()\n","print(std_r,std_g,std_b)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xO4yz5jk8Bx","executionInfo":{"status":"ok","timestamp":1731368153407,"user_tz":-60,"elapsed":9469,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"876371ff-1847-491f-a4ee-a1f7dce71ce5"},"id":"4xO4yz5jk8Bx","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","0.49139968 0.48215827 0.44653124\n","0.24703233 0.24348505 0.26158768\n"]}]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),            # Random crop with 4-pixel padding\n","    transforms.RandomHorizontalFlip(),               # Horizontal flip (50% prob it happens)\n","    transforms.ToTensor(),                           # Convert to tensor\n","    # To ensure zero centered input data\n","    transforms.Normalize((0.49139968 , 0.48215827 , 0.44653124),   # CIFAR-10 mean for each channel\n","                         (0.24703233 , 0.243, 0.24348505))      # CIFAR-10 std deviation for each channel\n","])\n","\n","# It's important to apply normalization on test to ensure consistency, ofc no augmentation instead\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.49139968 , 0.48215827 , 0.44653124),   # CIFAR-10 mean for each channel\n","                         (0.24703233 , 0.243, 0.24348505))      # CIFAR-10 std deviation for each channel\n","])\n","\n","# Load the CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Create data loaders\n","batch_size = 64\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader  = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0bJt4JdfZB8","executionInfo":{"status":"ok","timestamp":1731368235007,"user_tz":-60,"elapsed":1629,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"95a4047f-424c-470c-d4e5-9b134ad46974"},"id":"d0bJt4JdfZB8","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","id":"4486bead","metadata":{"id":"4486bead"},"source":["--------------------------------------------\n","\n","**Fill in the code in `libs/models/custom_cnn.py**\n","\n","--------------------------------------------"]},{"cell_type":"code","execution_count":34,"id":"30575ec4","metadata":{"id":"30575ec4","executionInfo":{"status":"ok","timestamp":1731368240018,"user_tz":-60,"elapsed":328,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}}},"outputs":[],"source":["# /content/drive/MyDrive/Homework02/libs/models/custom_cnn.py\n","import sys\n","sys.path.append('/content/drive/MyDrive/Homework02/libs/models')\n","\n","from custom_cnn import CustomCNN\n","\n","\n","net = CustomCNN().to(device)"]},{"cell_type":"markdown","id":"ecba4705","metadata":{"id":"ecba4705"},"source":["#### Training"]},{"cell_type":"code","execution_count":35,"id":"f0ba8675","metadata":{"id":"f0ba8675","executionInfo":{"status":"ok","timestamp":1731368405878,"user_tz":-60,"elapsed":164095,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9adfea75-8fbd-4f56-db70-3df15932be9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.3770\n","Epoch 2, Loss: 1.0499\n","Epoch 3, Loss: 0.9310\n","Epoch 4, Loss: 0.8560\n","Epoch 5, Loss: 0.8070\n","Epoch 6, Loss: 0.7548\n","Epoch 7, Loss: 0.7170\n","Epoch 8, Loss: 0.6804\n","Epoch 9, Loss: 0.6448\n","Epoch 10, Loss: 0.6177\n","Finished Training\n"]}],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=5e-05)\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    net.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n","\n","print('Finished Training')\n"]},{"cell_type":"markdown","id":"350ceb57","metadata":{"id":"350ceb57"},"source":["#### Evaluation"]},{"cell_type":"code","execution_count":36,"id":"fd7550f6","metadata":{"id":"fd7550f6","executionInfo":{"status":"ok","timestamp":1731368431043,"user_tz":-60,"elapsed":2126,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2810af96-c51d-4b15-e710-080ea2b96e1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on the test images: 77.08%\n"]}],"source":["# Evaluate the model\n","net.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"]},{"cell_type":"markdown","id":"f0c58dfb","metadata":{"id":"f0c58dfb"},"source":["\n","### 5.2: Pretrained network\n","In this exercise, you will start from scratch to adapt a pre-trained AlexNet model for the CIFAR-10 dataset.\n","\n","Instructions\n","\n","- Use torchvision.models to load a pre-trained AlexNet. Be sure to specify that the model should be pre-trained on ImageNet.\n","\n","- The CIFAR-10 dataset has 10 classes, so you need to update the model’s final layer to output 10 classes instead of the default 1000.\n","\n","- Replace the final fully connected layer in AlexNet’s classifier to output 10 classes.\n","\n","- To perform fine-tuning, freeze all layers except the newly added fully connected layer.\n","\n","- Move your model to the appropriate device (cuda if available). Define a device and ensure the model is moved to that device.\n"]},{"cell_type":"code","execution_count":5,"id":"ba5ee586","metadata":{"id":"ba5ee586","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731365299897,"user_tz":-60,"elapsed":2537,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"cdb96b59-ee26-4415-f05f-596d38ba5745"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# Libraries I need to start the code without rerunning the whole notebook\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# Define transformations for the data\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load the CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Create data loaders\n","batch_size = 64\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","source":["# Define a device and ensure the model is moved to that device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"3C5JokL6apsu","executionInfo":{"status":"ok","timestamp":1731365348590,"user_tz":-60,"elapsed":322,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}}},"id":"3C5JokL6apsu","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Debugging info"],"metadata":{"id":"s9lzfrcCpj8H"},"id":"s9lzfrcCpj8H"},{"cell_type":"code","source":["model = models.alexnet(pretrained=True) # pretrained=True ensure to use the weights pre-trained on the ImageNet dataset"],"metadata":{"id":"0Hsusuxsp9i8","executionInfo":{"status":"ok","timestamp":1731369362949,"user_tz":-60,"elapsed":1187,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}}},"id":"0Hsusuxsp9i8","execution_count":40,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pleGejTp_E7","executionInfo":{"status":"ok","timestamp":1731369420770,"user_tz":-60,"elapsed":493,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"4ca9fc13-2b14-4adc-f35f-dc3b7056e85b"},"id":"3pleGejTp_E7","execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["### Differences between my structure vs AlexNet\n","- AlexNet: Has 5 convolutional layers, mine has 4\n","- Initial large kernel (11x11) to focus on higher-level features early on\n","- Stride of 4 in the first layer aggressively reduce spatial dimensions initally\n","- I used Max Pooling after each convolutional layer, while AlexNet uses 2 in early layers and 1 in the end\n","- In the final output I used AdaptiveAvgPool2d\n","- I also added Batch Normalization after each convolutional layer\n","- AlexNet use a higher dropout rate of 0.5 (mine is 0.3)\n","\n","I think my solution provides a really efficient structure for CIFAR-10 dataset, more suitable for this simple dataset even if training was not enough cause Loss was func was still going down. On the other end, AlexNext leverage on his complexity and super tuned weights."],"metadata":{"id":"PSUA7gW_rBpe"},"id":"PSUA7gW_rBpe"},{"cell_type":"code","execution_count":9,"id":"32b8400e","metadata":{"id":"32b8400e","executionInfo":{"status":"ok","timestamp":1731365353590,"user_tz":-60,"elapsed":2495,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models, transforms, datasets\n","\n","# Modify the final layer of AlexNet for 10 classes instead of 1000\n","# so model.classifier[6] is the final fully connected layer\n","model.classifier[6] = nn.Linear(in_features=model.classifier[6].in_features, out_features=10) # we keep input features as they are so the layer structure remains compatible with the rest of the network but we change output to 10 classes (for our need)\n","\n","# Freeze all layers except the final fully connected layer\n","for param in model.parameters():\n","    param.requires_grad = False # allow to fix (so not update) layers's weights during backpropagation\n","for param in model.classifier[6].parameters():\n","    param.requires_grad = True # instead ofc we train the last one\n","\n","# This approach makes the train faster while reduce overfitting since we have trained on a bigger dataset than CIFAR-10\n","\n","model = model.to(device) # Move model to device\n"]},{"cell_type":"code","execution_count":11,"id":"09bfc3c4","metadata":{"id":"09bfc3c4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731365585890,"user_tz":-60,"elapsed":173982,"user":{"displayName":"Manu_Iaccarino 22","userId":"01411347728979905262"}},"outputId":"2d7a6d11-7d10-4f52-b6f0-393bff34e7b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.7283\n","Finished Training\n","Accuracy on the test images: 80.49%\n"]}],"source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-05)\n","\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}')\n","\n","print('Finished Training')\n","\n","# Evaluate the model\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy on the test images: {100 * correct / total:.2f}%')"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}